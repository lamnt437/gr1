{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lamnt\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\lamnt\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\lamnt\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\lamnt\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\lamnt\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\lamnt\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\lamnt\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\lamnt\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\lamnt\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\lamnt\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\lamnt\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\lamnt\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "NUM_CLASSES = 20\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        self._vocab_size = vocab_size\n",
    "        self._hidden_size = hidden_size\n",
    "        \n",
    "    def build_graph(self):\n",
    "        self._X = tf.placeholder(tf.float32, shape=[None, self._vocab_size])\n",
    "        self._real_Y = tf.placeholder(tf.int32, shape=[None, ])\n",
    "        \n",
    "        weights_1 = tf.get_variable(\n",
    "            name='weights_input_hidden',\n",
    "            shape=(self._vocab_size, self._hidden_size),\n",
    "            initializer=tf.random_normal_initializer(seed=2018)\n",
    "        )\n",
    "        \n",
    "        biases_1 = tf.get_variable(\n",
    "            name='biases_input_hidden',\n",
    "            shape=(self._hidden_size),\n",
    "            initializer=tf.random_normal_initializer(seed=2018)\n",
    "        )\n",
    "        \n",
    "        weights_2 = tf.get_variable(\n",
    "            name='weights_hidden_output',\n",
    "            shape=(self._hidden_size, NUM_CLASSES),\n",
    "            initializer=tf.random_normal_initializer(seed=2018)\n",
    "        )\n",
    "        \n",
    "        biases_2 = tf.get_variable(\n",
    "            name='biases_hidden_output',\n",
    "            shape=(NUM_CLASSES),\n",
    "            initializer=tf.random_normal_initializer(seed=2018)\n",
    "        )\n",
    "        \n",
    "        hidden = tf.matmul(self._X, weights_1) + biases_1\n",
    "        hidden = tf.sigmoid(hidden)\n",
    "        logits = tf.matmul(hidden, weights_2) + biases_2\n",
    "        \n",
    "        labels_one_hot = tf.one_hot(indices=self._real_Y, depth=NUM_CLASSES, dtype=tf.float32)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels_one_hot, logits=logits)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        \n",
    "        probs = tf.nn.softmax(logits)\n",
    "        predicted_labels = tf.argmax(probs, axis=1)\n",
    "        predicted_labels = tf.squeeze(predicted_labels)\n",
    "        \n",
    "        return predicted_labels, loss\n",
    "    \n",
    "    def trainer(self, loss, learning_rate):\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "        return train_op\n",
    "    \n",
    "\n",
    "class DataReader:\n",
    "    def __init__(self, data_path, batch_size, vocab_size):\n",
    "        self._batch_size = batch_size\n",
    "        with open(data_path) as f:\n",
    "            d_lines = f.read().splitlines()\n",
    "            \n",
    "        self._data = []\n",
    "        self._labels = []\n",
    "        for data_id, line in enumerate(d_lines):\n",
    "            vector = [0.0 for _ in range(vocab_size)]\n",
    "            features = line.split('<fff>')\n",
    "            label, doc_id = int(features[0]), int(features[1])\n",
    "            tokens = features[2].split()\n",
    "            for token in tokens:\n",
    "                index, value = int(token.split(':')[0]), float(token.split(':')[1])\n",
    "                vector[index] = value\n",
    "            self._data.append(vector)\n",
    "            self._labels.append(label)\n",
    "        \n",
    "        self._data = np.array(self._data)\n",
    "        self._labels = np.array(self._labels)\n",
    "        \n",
    "        self._num_epoch = 0\n",
    "        self._batch_id = 0\n",
    "            \n",
    "        \n",
    "    def next_batch(self):\n",
    "        start = self._batch_id * self._batch_size\n",
    "        end = start + self._batch_size\n",
    "        self._batch_id += 1\n",
    "        \n",
    "        if end + self._batch_size > len(self._data):\n",
    "#         if True:\n",
    "            end = len(self._data)\n",
    "            self._num_epoch += 1\n",
    "            self._batch_id = 0\n",
    "            indices = list(range(len(self._data)))\n",
    "            random.seed(2018)\n",
    "            random.shuffle(indices)\n",
    "            self._data, self._labels = self._data[indices], self._labels[indices]\n",
    "        \n",
    "        return self._data[start:end], self._labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-fd211063e4df>:44: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "step: 1, loss: 0.5065922737121582\n",
      "step: 2, loss: 0.00016962051449809223\n",
      "step: 3, loss: 8.225404144468484e-07\n",
      "step: 4, loss: 4.291533528544278e-08\n",
      "step: 5, loss: 0.0\n",
      "step: 6, loss: 0.0\n",
      "step: 7, loss: 0.0\n",
      "step: 8, loss: 0.0\n",
      "step: 9, loss: 0.0\n",
      "step: 10, loss: 15.89012336730957\n",
      "step: 11, loss: 36.74134063720703\n",
      "step: 12, loss: 32.45412063598633\n",
      "step: 13, loss: 26.995887756347656\n",
      "step: 14, loss: 20.358125686645508\n",
      "step: 15, loss: 12.29566764831543\n",
      "step: 16, loss: 5.123974800109863\n",
      "step: 17, loss: 0.7438090443611145\n",
      "step: 18, loss: 0.01452797744423151\n",
      "step: 19, loss: 1.0787778592202812e-05\n",
      "step: 20, loss: 7.671380444662645e-06\n",
      "step: 21, loss: 2.384184938364342e-08\n",
      "step: 22, loss: 21.03221893310547\n",
      "step: 23, loss: 27.945518493652344\n",
      "step: 24, loss: 26.411508560180664\n",
      "step: 25, loss: 23.422630310058594\n",
      "step: 26, loss: 20.061948776245117\n",
      "step: 27, loss: 16.139047622680664\n",
      "step: 28, loss: 12.28774642944336\n",
      "step: 29, loss: 8.550832748413086\n",
      "step: 30, loss: 5.314810276031494\n",
      "step: 31, loss: 1.555302619934082\n",
      "step: 32, loss: 0.1193101704120636\n",
      "step: 33, loss: 0.007487305905669928\n",
      "step: 34, loss: 22.43597984313965\n",
      "step: 35, loss: 24.86786651611328\n",
      "step: 36, loss: 23.149721145629883\n",
      "step: 37, loss: 21.22954559326172\n",
      "step: 38, loss: 18.320775985717773\n",
      "step: 39, loss: 14.988089561462402\n",
      "step: 40, loss: 11.524730682373047\n",
      "step: 41, loss: 8.67182731628418\n",
      "step: 42, loss: 5.836905479431152\n",
      "step: 43, loss: 3.700211524963379\n",
      "step: 44, loss: 1.0730490684509277\n",
      "step: 45, loss: 1.4161709547042847\n",
      "step: 46, loss: 12.535536766052246\n",
      "step: 47, loss: 12.524704933166504\n",
      "step: 48, loss: 12.147313117980957\n",
      "step: 49, loss: 10.675695419311523\n",
      "step: 50, loss: 9.370983123779297\n",
      "step: 51, loss: 7.2466044425964355\n",
      "step: 52, loss: 5.3705668449401855\n",
      "step: 53, loss: 3.48752498626709\n",
      "step: 54, loss: 1.3660660982131958\n",
      "step: 55, loss: 0.6675176024436951\n",
      "step: 56, loss: 0.15719132125377655\n",
      "step: 57, loss: 6.682583808898926\n",
      "step: 58, loss: 11.230854034423828\n",
      "step: 59, loss: 11.220246315002441\n",
      "step: 60, loss: 9.473886489868164\n",
      "step: 61, loss: 7.74034309387207\n",
      "step: 62, loss: 5.990378379821777\n",
      "step: 63, loss: 4.284997463226318\n",
      "step: 64, loss: 2.564138412475586\n",
      "step: 65, loss: 1.5252031087875366\n",
      "step: 66, loss: 0.7429736852645874\n",
      "step: 67, loss: 0.3203991949558258\n",
      "step: 68, loss: 0.17116989195346832\n",
      "step: 69, loss: 11.22673225402832\n",
      "step: 70, loss: 15.66214370727539\n",
      "step: 71, loss: 14.546243667602539\n",
      "step: 72, loss: 13.528611183166504\n",
      "step: 73, loss: 12.014522552490234\n",
      "step: 74, loss: 10.0939359664917\n",
      "step: 75, loss: 9.357427597045898\n",
      "step: 76, loss: 8.595841407775879\n",
      "step: 77, loss: 7.113119125366211\n",
      "step: 78, loss: 6.4551262855529785\n",
      "step: 79, loss: 5.418879508972168\n",
      "step: 80, loss: 4.880774974822998\n",
      "step: 81, loss: 2.735424280166626\n",
      "step: 82, loss: 2.309490919113159\n",
      "step: 83, loss: 1.9764293432235718\n",
      "step: 84, loss: 1.7169215679168701\n",
      "step: 85, loss: 1.440862774848938\n",
      "step: 86, loss: 1.222282886505127\n",
      "step: 87, loss: 1.08262038230896\n",
      "step: 88, loss: 0.8898847103118896\n",
      "step: 89, loss: 0.7994129061698914\n",
      "step: 90, loss: 0.6346768140792847\n",
      "step: 91, loss: 0.5469166040420532\n",
      "step: 92, loss: 1.531589388847351\n",
      "step: 93, loss: 10.401310920715332\n",
      "step: 94, loss: 10.095968246459961\n",
      "step: 95, loss: 10.032344818115234\n",
      "step: 96, loss: 9.729364395141602\n",
      "step: 97, loss: 9.625651359558105\n",
      "step: 98, loss: 9.284126281738281\n",
      "step: 99, loss: 9.10738754272461\n",
      "step: 100, loss: 8.84327507019043\n",
      "step: 101, loss: 8.457265853881836\n",
      "step: 102, loss: 8.200891494750977\n",
      "step: 103, loss: 7.9144792556762695\n",
      "step: 104, loss: 8.2040433883667\n",
      "step: 105, loss: 10.918750762939453\n",
      "step: 106, loss: 10.609539985656738\n",
      "step: 107, loss: 10.410043716430664\n",
      "step: 108, loss: 10.155449867248535\n",
      "step: 109, loss: 9.706480979919434\n",
      "step: 110, loss: 9.6072416305542\n",
      "step: 111, loss: 9.12187671661377\n",
      "step: 112, loss: 8.808459281921387\n",
      "step: 113, loss: 8.415221214294434\n",
      "step: 114, loss: 8.152514457702637\n",
      "step: 115, loss: 7.794190883636475\n",
      "step: 116, loss: 7.8804402351379395\n",
      "step: 117, loss: 9.809847831726074\n",
      "step: 118, loss: 9.58854866027832\n",
      "step: 119, loss: 9.265798568725586\n",
      "step: 120, loss: 9.114261627197266\n",
      "step: 121, loss: 8.771768569946289\n",
      "step: 122, loss: 8.474205017089844\n",
      "step: 123, loss: 8.110274314880371\n",
      "step: 124, loss: 7.813999176025391\n",
      "step: 125, loss: 7.497314929962158\n",
      "step: 126, loss: 7.080047130584717\n",
      "step: 127, loss: 6.6588287353515625\n",
      "step: 128, loss: 7.1444315910339355\n",
      "step: 129, loss: 10.915785789489746\n",
      "step: 130, loss: 10.724808692932129\n",
      "step: 131, loss: 10.523752212524414\n",
      "step: 132, loss: 10.238100051879883\n",
      "step: 133, loss: 9.969005584716797\n",
      "step: 134, loss: 9.654474258422852\n",
      "step: 135, loss: 9.344058990478516\n",
      "step: 136, loss: 8.973448753356934\n",
      "step: 137, loss: 8.555008888244629\n",
      "step: 138, loss: 8.157859802246094\n",
      "step: 139, loss: 7.629168510437012\n",
      "step: 140, loss: 7.163512706756592\n",
      "step: 141, loss: 6.829927444458008\n",
      "step: 142, loss: 6.357855319976807\n",
      "step: 143, loss: 5.8784918785095215\n",
      "step: 144, loss: 5.310401439666748\n",
      "step: 145, loss: 4.78953218460083\n",
      "step: 146, loss: 4.143805980682373\n",
      "step: 147, loss: 3.5462050437927246\n",
      "step: 148, loss: 2.828380823135376\n",
      "step: 149, loss: 2.1580147743225098\n",
      "step: 150, loss: 1.4759236574172974\n",
      "step: 151, loss: 0.9945492744445801\n",
      "step: 152, loss: 7.324478626251221\n",
      "step: 153, loss: 14.425097465515137\n",
      "step: 154, loss: 14.184802055358887\n",
      "step: 155, loss: 13.871115684509277\n",
      "step: 156, loss: 13.446516036987305\n",
      "step: 157, loss: 12.977805137634277\n",
      "step: 158, loss: 12.535589218139648\n",
      "step: 159, loss: 11.950403213500977\n",
      "step: 160, loss: 11.261638641357422\n",
      "step: 161, loss: 10.53704833984375\n",
      "step: 162, loss: 9.827522277832031\n",
      "step: 163, loss: 9.049751281738281\n",
      "step: 164, loss: 10.676712989807129\n",
      "step: 165, loss: 11.929067611694336\n",
      "step: 166, loss: 11.236684799194336\n",
      "step: 167, loss: 10.5748929977417\n",
      "step: 168, loss: 9.901698112487793\n",
      "step: 169, loss: 9.218552589416504\n",
      "step: 170, loss: 8.456897735595703\n",
      "step: 171, loss: 7.7182183265686035\n",
      "step: 172, loss: 6.927423000335693\n",
      "step: 173, loss: 6.216951370239258\n",
      "step: 174, loss: 5.479646682739258\n",
      "step: 175, loss: 4.784750461578369\n",
      "step: 176, loss: 7.589205741882324\n",
      "step: 177, loss: 8.19450855255127\n",
      "step: 178, loss: 7.720004081726074\n",
      "step: 179, loss: 7.202736854553223\n",
      "step: 180, loss: 6.65648078918457\n",
      "step: 181, loss: 6.128012657165527\n",
      "step: 182, loss: 5.6091790199279785\n",
      "step: 183, loss: 5.0661091804504395\n",
      "step: 184, loss: 4.51763391494751\n",
      "step: 185, loss: 3.984917402267456\n",
      "step: 186, loss: 3.6317191123962402\n",
      "step: 187, loss: 3.0614984035491943\n",
      "step: 188, loss: 11.291512489318848\n",
      "step: 189, loss: 13.738718032836914\n",
      "step: 190, loss: 13.43740463256836\n",
      "step: 191, loss: 12.848148345947266\n",
      "step: 192, loss: 12.392982482910156\n",
      "step: 193, loss: 11.541298866271973\n",
      "step: 194, loss: 10.265812873840332\n",
      "step: 195, loss: 9.636382102966309\n",
      "step: 196, loss: 8.697049140930176\n",
      "step: 197, loss: 7.438502788543701\n",
      "step: 198, loss: 6.622798442840576\n",
      "step: 199, loss: 6.910754203796387\n",
      "step: 200, loss: 6.883342742919922\n",
      "step: 201, loss: 6.687872886657715\n",
      "step: 202, loss: 6.445817947387695\n",
      "step: 203, loss: 6.221673965454102\n",
      "step: 204, loss: 5.911452770233154\n",
      "step: 205, loss: 5.685718536376953\n",
      "step: 206, loss: 5.402523040771484\n",
      "step: 207, loss: 4.948403835296631\n",
      "step: 208, loss: 4.588021755218506\n",
      "step: 209, loss: 4.0620341300964355\n",
      "step: 210, loss: 6.556845664978027\n",
      "step: 211, loss: 8.609427452087402\n",
      "step: 212, loss: 8.594303131103516\n",
      "step: 213, loss: 8.031213760375977\n",
      "step: 214, loss: 7.9975385665893555\n",
      "step: 215, loss: 7.561263561248779\n",
      "step: 216, loss: 7.247493267059326\n",
      "step: 217, loss: 6.993365287780762\n",
      "step: 218, loss: 6.598743915557861\n",
      "step: 219, loss: 6.4669413566589355\n",
      "step: 220, loss: 7.047733306884766\n",
      "step: 221, loss: 6.701315402984619\n",
      "step: 222, loss: 6.456542491912842\n",
      "step: 223, loss: 6.169682502746582\n",
      "step: 224, loss: 5.914991855621338\n",
      "step: 225, loss: 5.554919242858887\n",
      "step: 226, loss: 3.368809223175049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 227, loss: 3.8068840503692627\n",
      "step: 228, loss: 3.403703212738037\n",
      "step: 229, loss: 3.389796733856201\n",
      "step: 230, loss: 3.326944351196289\n",
      "step: 231, loss: 3.270170211791992\n",
      "step: 232, loss: 3.1929895877838135\n",
      "step: 233, loss: 3.232067823410034\n",
      "step: 234, loss: 3.0102570056915283\n",
      "step: 235, loss: 3.5831003189086914\n",
      "step: 236, loss: 3.025265693664551\n",
      "step: 237, loss: 3.2527036666870117\n",
      "step: 238, loss: 2.8983349800109863\n",
      "step: 239, loss: 2.850038528442383\n",
      "step: 240, loss: 2.7265963554382324\n",
      "step: 241, loss: 2.556800127029419\n",
      "step: 242, loss: 2.666511297225952\n",
      "step: 243, loss: 2.7543182373046875\n",
      "step: 244, loss: 2.719313144683838\n",
      "step: 245, loss: 2.4614033699035645\n",
      "step: 246, loss: 2.3685076236724854\n",
      "step: 247, loss: 2.9488136768341064\n",
      "step: 248, loss: 2.673924207687378\n",
      "step: 249, loss: 2.2199668884277344\n",
      "step: 250, loss: 2.404661178588867\n",
      "step: 251, loss: 2.181959867477417\n",
      "step: 252, loss: 2.3317389488220215\n",
      "step: 253, loss: 2.13496994972229\n",
      "step: 254, loss: 2.0438971519470215\n",
      "step: 255, loss: 2.2624623775482178\n",
      "step: 256, loss: 2.5544257164001465\n",
      "step: 257, loss: 2.4703290462493896\n",
      "step: 258, loss: 2.4956207275390625\n",
      "step: 259, loss: 2.2722222805023193\n",
      "step: 260, loss: 2.1762053966522217\n",
      "step: 261, loss: 2.006791114807129\n",
      "step: 262, loss: 2.4321908950805664\n",
      "step: 263, loss: 1.8075506687164307\n",
      "step: 264, loss: 2.074690580368042\n",
      "step: 265, loss: 2.2558188438415527\n",
      "step: 266, loss: 1.7348278760910034\n",
      "step: 267, loss: 1.726576328277588\n",
      "step: 268, loss: 1.7228549718856812\n",
      "step: 269, loss: 1.6504484415054321\n",
      "step: 270, loss: 2.3856358528137207\n",
      "step: 271, loss: 2.0007896423339844\n",
      "step: 272, loss: 1.453432559967041\n",
      "step: 273, loss: 1.8043867349624634\n",
      "step: 274, loss: 1.6320652961730957\n",
      "step: 275, loss: 1.939246654510498\n",
      "step: 276, loss: 1.6133239269256592\n",
      "step: 277, loss: 1.8259965181350708\n",
      "step: 278, loss: 1.5278147459030151\n",
      "step: 279, loss: 1.7980046272277832\n",
      "step: 280, loss: 1.6292091608047485\n",
      "step: 281, loss: 1.5408241748809814\n",
      "step: 282, loss: 1.7551641464233398\n",
      "step: 283, loss: 1.740790843963623\n",
      "step: 284, loss: 1.4178317785263062\n",
      "step: 285, loss: 1.642806053161621\n",
      "step: 286, loss: 1.6220147609710693\n",
      "step: 287, loss: 1.332241415977478\n",
      "step: 288, loss: 1.5940008163452148\n",
      "step: 289, loss: 1.6226500272750854\n",
      "step: 290, loss: 1.5140265226364136\n",
      "step: 291, loss: 1.5583100318908691\n",
      "step: 292, loss: 1.8108608722686768\n",
      "step: 293, loss: 1.3745083808898926\n",
      "step: 294, loss: 1.141188144683838\n",
      "step: 295, loss: 0.9737207889556885\n",
      "step: 296, loss: 1.594849705696106\n",
      "step: 297, loss: 0.9324093461036682\n",
      "step: 298, loss: 1.2592884302139282\n",
      "step: 299, loss: 1.4292219877243042\n",
      "step: 300, loss: 1.2339656352996826\n",
      "step: 301, loss: 1.1334799528121948\n",
      "step: 302, loss: 1.3876707553863525\n",
      "step: 303, loss: 1.0585440397262573\n",
      "step: 304, loss: 0.9621917605400085\n",
      "step: 305, loss: 1.5390870571136475\n",
      "step: 306, loss: 1.0045830011367798\n",
      "step: 307, loss: 1.0864663124084473\n",
      "step: 308, loss: 1.1824382543563843\n",
      "step: 309, loss: 0.7577391862869263\n",
      "step: 310, loss: 1.1366829872131348\n",
      "step: 311, loss: 0.8159258365631104\n",
      "step: 312, loss: 0.533240556716919\n",
      "step: 313, loss: 1.0274449586868286\n",
      "step: 314, loss: 0.9099565744400024\n",
      "step: 315, loss: 1.5365971326828003\n",
      "step: 316, loss: 1.1758129596710205\n",
      "step: 317, loss: 1.27741539478302\n",
      "step: 318, loss: 0.7603734731674194\n",
      "step: 319, loss: 0.88443922996521\n",
      "step: 320, loss: 1.022122859954834\n",
      "step: 321, loss: 0.7886402606964111\n",
      "step: 322, loss: 0.8519861698150635\n",
      "step: 323, loss: 1.2788783311843872\n",
      "step: 324, loss: 0.6856659054756165\n",
      "step: 325, loss: 1.2472609281539917\n",
      "step: 326, loss: 1.0477750301361084\n",
      "step: 327, loss: 0.8867909908294678\n",
      "step: 328, loss: 1.128638744354248\n",
      "step: 329, loss: 0.7857747077941895\n",
      "step: 330, loss: 1.3703433275222778\n",
      "step: 331, loss: 0.9460490942001343\n",
      "step: 332, loss: 0.7435668110847473\n",
      "step: 333, loss: 1.0342918634414673\n",
      "step: 334, loss: 1.112808108329773\n",
      "step: 335, loss: 0.6915059685707092\n",
      "step: 336, loss: 0.8690657019615173\n",
      "step: 337, loss: 0.7087562680244446\n",
      "step: 338, loss: 1.0954409837722778\n",
      "step: 339, loss: 0.6830586194992065\n",
      "step: 340, loss: 0.5490718483924866\n",
      "step: 341, loss: 1.071486473083496\n",
      "step: 342, loss: 0.5022901892662048\n",
      "step: 343, loss: 1.05093252658844\n",
      "step: 344, loss: 0.7965237498283386\n",
      "step: 345, loss: 0.8424039483070374\n",
      "step: 346, loss: 0.5869194269180298\n",
      "step: 347, loss: 0.6980693936347961\n",
      "step: 348, loss: 0.9449962377548218\n",
      "step: 349, loss: 0.9195663332939148\n",
      "step: 350, loss: 1.0323190689086914\n",
      "step: 351, loss: 0.5058678388595581\n",
      "step: 352, loss: 0.6981335282325745\n",
      "step: 353, loss: 0.8891154527664185\n",
      "step: 354, loss: 0.713188886642456\n",
      "step: 355, loss: 0.792738139629364\n",
      "step: 356, loss: 0.41534876823425293\n",
      "step: 357, loss: 0.6745700240135193\n",
      "step: 358, loss: 0.9405811429023743\n",
      "step: 359, loss: 0.6204662919044495\n",
      "step: 360, loss: 0.5917358994483948\n",
      "step: 361, loss: 1.0564568042755127\n",
      "step: 362, loss: 0.9981065392494202\n",
      "step: 363, loss: 0.5625764727592468\n",
      "step: 364, loss: 0.4449877142906189\n",
      "step: 365, loss: 1.1254053115844727\n",
      "step: 366, loss: 0.6119056940078735\n",
      "step: 367, loss: 0.3785354197025299\n",
      "step: 368, loss: 0.9168668985366821\n",
      "step: 369, loss: 0.27817094326019287\n",
      "step: 370, loss: 0.5851263999938965\n",
      "step: 371, loss: 0.5438226461410522\n",
      "step: 372, loss: 0.5368478894233704\n",
      "step: 373, loss: 0.7321892380714417\n",
      "step: 374, loss: 0.7763354778289795\n",
      "step: 375, loss: 0.6283558011054993\n",
      "step: 376, loss: 0.7101907134056091\n",
      "step: 377, loss: 0.879822313785553\n",
      "step: 378, loss: 0.4624466300010681\n",
      "step: 379, loss: 0.7141475081443787\n",
      "step: 380, loss: 0.5806976556777954\n",
      "step: 381, loss: 0.7945543527603149\n",
      "step: 382, loss: 0.453763872385025\n",
      "step: 383, loss: 0.6353094577789307\n",
      "step: 384, loss: 0.3959900140762329\n",
      "step: 385, loss: 0.6043294668197632\n",
      "step: 386, loss: 0.33577144145965576\n",
      "step: 387, loss: 0.562734067440033\n",
      "step: 388, loss: 0.6944195628166199\n",
      "step: 389, loss: 0.626915693283081\n",
      "step: 390, loss: 0.9004088640213013\n",
      "step: 391, loss: 0.8353082537651062\n",
      "step: 392, loss: 0.4150778651237488\n",
      "step: 393, loss: 0.2736622095108032\n",
      "step: 394, loss: 0.5501020550727844\n",
      "step: 395, loss: 0.8077056407928467\n",
      "step: 396, loss: 0.7643826007843018\n",
      "step: 397, loss: 0.46383553743362427\n",
      "step: 398, loss: 0.4538418650627136\n",
      "step: 399, loss: 0.3580014705657959\n",
      "step: 400, loss: 0.6837104558944702\n",
      "step: 401, loss: 0.4023488759994507\n",
      "step: 402, loss: 0.7994690537452698\n",
      "step: 403, loss: 0.26181209087371826\n",
      "step: 404, loss: 0.8880932331085205\n",
      "step: 405, loss: 0.8862011432647705\n",
      "step: 406, loss: 0.7221981287002563\n",
      "step: 407, loss: 0.449375718832016\n",
      "step: 408, loss: 0.7256929278373718\n",
      "step: 409, loss: 0.5820648670196533\n",
      "step: 410, loss: 0.27458178997039795\n",
      "step: 411, loss: 0.5848293900489807\n",
      "step: 412, loss: 0.45466166734695435\n",
      "step: 413, loss: 0.45984184741973877\n",
      "step: 414, loss: 0.36527058482170105\n",
      "step: 415, loss: 0.4897429347038269\n",
      "step: 416, loss: 0.8937070369720459\n",
      "step: 417, loss: 1.0180970430374146\n",
      "step: 418, loss: 0.5978251695632935\n",
      "step: 419, loss: 0.3769218921661377\n",
      "step: 420, loss: 0.8317960500717163\n",
      "step: 421, loss: 0.28508731722831726\n",
      "step: 422, loss: 0.41802871227264404\n",
      "step: 423, loss: 0.43628355860710144\n",
      "step: 424, loss: 0.5803359746932983\n",
      "step: 425, loss: 0.277180552482605\n",
      "step: 426, loss: 0.40874671936035156\n",
      "step: 427, loss: 0.24612155556678772\n",
      "step: 428, loss: 0.9751715660095215\n",
      "step: 429, loss: 0.20844651758670807\n",
      "step: 430, loss: 0.5910886526107788\n",
      "step: 431, loss: 0.5663805603981018\n",
      "step: 432, loss: 0.47401610016822815\n",
      "step: 433, loss: 0.627065122127533\n",
      "step: 434, loss: 0.7562054991722107\n",
      "step: 435, loss: 0.5369436144828796\n",
      "step: 436, loss: 0.28972554206848145\n",
      "step: 437, loss: 0.7206993699073792\n",
      "step: 438, loss: 0.6316898465156555\n",
      "step: 439, loss: 0.5985056757926941\n",
      "step: 440, loss: 0.24650123715400696\n",
      "step: 441, loss: 0.27923041582107544\n",
      "step: 442, loss: 0.5406615734100342\n",
      "step: 443, loss: 0.44159016013145447\n",
      "step: 444, loss: 0.4082307815551758\n",
      "step: 445, loss: 0.3922358751296997\n",
      "step: 446, loss: 0.34756869077682495\n",
      "step: 447, loss: 0.610840916633606\n",
      "step: 448, loss: 0.3760285973548889\n",
      "step: 449, loss: 0.5459986925125122\n",
      "step: 450, loss: 0.7098940014839172\n",
      "step: 451, loss: 0.8584350347518921\n",
      "step: 452, loss: 0.11296620965003967\n",
      "step: 453, loss: 0.02608255296945572\n",
      "step: 454, loss: 0.0430435910820961\n",
      "step: 455, loss: 0.20191922783851624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 456, loss: 0.11112624406814575\n",
      "step: 457, loss: 0.16519871354103088\n",
      "step: 458, loss: 0.3228713870048523\n",
      "step: 459, loss: 0.13709202408790588\n",
      "step: 460, loss: 0.16988694667816162\n",
      "step: 461, loss: 0.14190511405467987\n",
      "step: 462, loss: 0.41463935375213623\n",
      "step: 463, loss: 0.08421148359775543\n",
      "step: 464, loss: 0.25556597113609314\n",
      "step: 465, loss: 0.14804629981517792\n",
      "step: 466, loss: 0.22594095766544342\n",
      "step: 467, loss: 0.1137775331735611\n",
      "step: 468, loss: 0.19310332834720612\n",
      "step: 469, loss: 0.2133554220199585\n",
      "step: 470, loss: 0.18512466549873352\n",
      "step: 471, loss: 0.5359179973602295\n",
      "step: 472, loss: 0.17072229087352753\n",
      "step: 473, loss: 0.1379334032535553\n",
      "step: 474, loss: 0.28487199544906616\n",
      "step: 475, loss: 0.17798402905464172\n",
      "step: 476, loss: 0.12628836929798126\n",
      "step: 477, loss: 0.17773181200027466\n",
      "step: 478, loss: 0.21798956394195557\n",
      "step: 479, loss: 0.2391718626022339\n",
      "step: 480, loss: 0.2787628769874573\n",
      "step: 481, loss: 0.148030087351799\n",
      "step: 482, loss: 0.04380195215344429\n",
      "step: 483, loss: 0.16812646389007568\n",
      "step: 484, loss: 0.23064620792865753\n",
      "step: 485, loss: 0.34290653467178345\n",
      "step: 486, loss: 0.18699736893177032\n",
      "step: 487, loss: 0.2631755471229553\n",
      "step: 488, loss: 0.11658839136362076\n",
      "step: 489, loss: 0.11809833347797394\n",
      "step: 490, loss: 0.14256936311721802\n",
      "step: 491, loss: 0.1544671207666397\n",
      "step: 492, loss: 0.1189858689904213\n",
      "step: 493, loss: 0.048890624195337296\n",
      "step: 494, loss: 0.08746694773435593\n",
      "step: 495, loss: 0.20184801518917084\n",
      "step: 496, loss: 0.23149709403514862\n",
      "step: 497, loss: 0.3963663578033447\n",
      "step: 498, loss: 0.2591063380241394\n",
      "step: 499, loss: 0.3367350399494171\n",
      "step: 500, loss: 0.08477558940649033\n",
      "step: 501, loss: 0.18093647062778473\n",
      "step: 502, loss: 0.24881118535995483\n",
      "step: 503, loss: 0.25217825174331665\n",
      "step: 504, loss: 0.225464329123497\n",
      "step: 505, loss: 0.06384292989969254\n",
      "step: 506, loss: 0.06559543311595917\n",
      "step: 507, loss: 0.1588292121887207\n",
      "step: 508, loss: 0.19591206312179565\n",
      "step: 509, loss: 0.16182617843151093\n",
      "step: 510, loss: 0.31570670008659363\n",
      "step: 511, loss: 0.23006212711334229\n",
      "step: 512, loss: 0.10663148015737534\n",
      "step: 513, loss: 0.38232922554016113\n",
      "step: 514, loss: 0.16902261972427368\n",
      "step: 515, loss: 0.11658399552106857\n",
      "step: 516, loss: 0.23479817807674408\n",
      "step: 517, loss: 0.21199199557304382\n",
      "step: 518, loss: 0.060531169176101685\n",
      "step: 519, loss: 0.17288745939731598\n",
      "step: 520, loss: 0.08293655514717102\n",
      "step: 521, loss: 0.09211350232362747\n",
      "step: 522, loss: 0.2511499226093292\n",
      "step: 523, loss: 0.04393494501709938\n",
      "step: 524, loss: 0.08868009597063065\n",
      "step: 525, loss: 0.30664440989494324\n",
      "step: 526, loss: 0.1471293568611145\n",
      "step: 527, loss: 0.12374982982873917\n",
      "step: 528, loss: 0.16821104288101196\n",
      "step: 529, loss: 0.17597322165966034\n",
      "step: 530, loss: 0.16026024520397186\n",
      "step: 531, loss: 0.27393263578414917\n",
      "step: 532, loss: 0.15269146859645844\n",
      "step: 533, loss: 0.20791679620742798\n",
      "step: 534, loss: 0.23785905539989471\n",
      "step: 535, loss: 0.14815855026245117\n",
      "step: 536, loss: 0.35358551144599915\n",
      "step: 537, loss: 0.16041317582130432\n",
      "step: 538, loss: 0.0834837332367897\n",
      "step: 539, loss: 0.06404760479927063\n",
      "step: 540, loss: 0.1046205684542656\n",
      "step: 541, loss: 0.0590701587498188\n",
      "step: 542, loss: 0.04241529852151871\n",
      "step: 543, loss: 0.2132229059934616\n",
      "step: 544, loss: 0.34720316529273987\n",
      "step: 545, loss: 0.2162846177816391\n",
      "step: 546, loss: 0.13033002614974976\n",
      "step: 547, loss: 0.1284051090478897\n",
      "step: 548, loss: 0.29833170771598816\n",
      "step: 549, loss: 0.3222379684448242\n",
      "step: 550, loss: 0.08565258979797363\n",
      "step: 551, loss: 0.2977491617202759\n",
      "step: 552, loss: 0.29599085450172424\n",
      "step: 553, loss: 0.06018516421318054\n",
      "step: 554, loss: 0.31571319699287415\n",
      "step: 555, loss: 0.20856450498104095\n",
      "step: 556, loss: 0.12513794004917145\n",
      "step: 557, loss: 0.28656038641929626\n",
      "step: 558, loss: 0.16870027780532837\n",
      "step: 559, loss: 0.2160869985818863\n",
      "step: 560, loss: 0.25552472472190857\n",
      "step: 561, loss: 0.18052852153778076\n",
      "step: 562, loss: 0.07045131921768188\n",
      "step: 563, loss: 0.08351044356822968\n",
      "step: 564, loss: 0.21240808069705963\n",
      "step: 565, loss: 0.1747799664735794\n",
      "step: 566, loss: 0.4793233871459961\n",
      "step: 567, loss: 0.3534894585609436\n",
      "step: 568, loss: 0.04065721854567528\n",
      "step: 569, loss: 0.0352884903550148\n",
      "step: 570, loss: 0.14748302102088928\n",
      "step: 571, loss: 0.19430603086948395\n",
      "step: 572, loss: 0.04276473820209503\n",
      "step: 573, loss: 0.2585914134979248\n",
      "step: 574, loss: 0.2779619097709656\n",
      "step: 575, loss: 0.1073329821228981\n",
      "step: 576, loss: 0.30277562141418457\n",
      "step: 577, loss: 0.1743726134300232\n",
      "step: 578, loss: 0.1898398995399475\n",
      "step: 579, loss: 0.31915926933288574\n",
      "step: 580, loss: 0.2184024304151535\n",
      "step: 581, loss: 0.21125301718711853\n",
      "step: 582, loss: 0.186029314994812\n",
      "step: 583, loss: 0.14460521936416626\n",
      "step: 584, loss: 0.2307509034872055\n",
      "step: 585, loss: 0.2273712009191513\n",
      "step: 586, loss: 0.2442295402288437\n",
      "step: 587, loss: 0.13303761184215546\n",
      "step: 588, loss: 0.16129052639007568\n",
      "step: 589, loss: 0.25207188725471497\n",
      "step: 590, loss: 0.07155539095401764\n",
      "step: 591, loss: 0.4916624426841736\n",
      "step: 592, loss: 0.02453026734292507\n",
      "step: 593, loss: 0.09698940068483353\n",
      "step: 594, loss: 0.0957041084766388\n",
      "step: 595, loss: 0.16597925126552582\n",
      "step: 596, loss: 0.04857092723250389\n",
      "step: 597, loss: 0.13202977180480957\n",
      "step: 598, loss: 0.2704921066761017\n",
      "step: 599, loss: 0.17158269882202148\n",
      "step: 600, loss: 0.1027442142367363\n",
      "step: 601, loss: 0.15989671647548676\n",
      "step: 602, loss: 0.14582513272762299\n",
      "step: 603, loss: 0.2899860739707947\n",
      "step: 604, loss: 0.1524406522512436\n",
      "step: 605, loss: 0.08269179612398148\n",
      "step: 606, loss: 0.0669248029589653\n",
      "step: 607, loss: 0.26770880818367004\n",
      "step: 608, loss: 0.17348438501358032\n",
      "step: 609, loss: 0.20606382191181183\n",
      "step: 610, loss: 0.2556860148906708\n",
      "step: 611, loss: 0.23678240180015564\n",
      "step: 612, loss: 0.2739904820919037\n",
      "step: 613, loss: 0.09928100556135178\n",
      "step: 614, loss: 0.20686110854148865\n",
      "step: 615, loss: 0.0861370861530304\n",
      "step: 616, loss: 0.3723895251750946\n",
      "step: 617, loss: 0.20073851943016052\n",
      "step: 618, loss: 0.14059528708457947\n",
      "step: 619, loss: 0.06765037775039673\n",
      "step: 620, loss: 0.11078830808401108\n",
      "step: 621, loss: 0.2709693908691406\n",
      "step: 622, loss: 0.3457307517528534\n",
      "step: 623, loss: 0.11547259241342545\n",
      "step: 624, loss: 0.31449639797210693\n",
      "step: 625, loss: 0.07933956384658813\n",
      "step: 626, loss: 0.1987798810005188\n",
      "step: 627, loss: 0.1841767132282257\n",
      "step: 628, loss: 0.08432245254516602\n",
      "step: 629, loss: 0.28379982709884644\n",
      "step: 630, loss: 0.16223397850990295\n",
      "step: 631, loss: 0.14451831579208374\n",
      "step: 632, loss: 0.0743027999997139\n",
      "step: 633, loss: 0.29897868633270264\n",
      "step: 634, loss: 0.09563298523426056\n",
      "step: 635, loss: 0.12949307262897491\n",
      "step: 636, loss: 0.2802310585975647\n",
      "step: 637, loss: 0.11929485201835632\n",
      "step: 638, loss: 0.22824472188949585\n",
      "step: 639, loss: 0.16198210418224335\n",
      "step: 640, loss: 0.13790756464004517\n",
      "step: 641, loss: 0.03877676650881767\n",
      "step: 642, loss: 0.28879931569099426\n",
      "step: 643, loss: 0.1492728292942047\n",
      "step: 644, loss: 0.05269454047083855\n",
      "step: 645, loss: 0.2960700988769531\n",
      "step: 646, loss: 0.09438575804233551\n",
      "step: 647, loss: 0.17106232047080994\n",
      "step: 648, loss: 0.12447479367256165\n",
      "step: 649, loss: 0.16707010567188263\n",
      "step: 650, loss: 0.08111821115016937\n",
      "step: 651, loss: 0.2269943356513977\n",
      "step: 652, loss: 0.15547141432762146\n",
      "step: 653, loss: 0.10772930830717087\n",
      "step: 654, loss: 0.08650688081979752\n",
      "step: 655, loss: 0.25903409719467163\n",
      "step: 656, loss: 0.26636019349098206\n",
      "step: 657, loss: 0.18566909432411194\n",
      "step: 658, loss: 0.23935186862945557\n",
      "step: 659, loss: 0.14395862817764282\n",
      "step: 660, loss: 0.26905110478401184\n",
      "step: 661, loss: 0.7193188667297363\n",
      "step: 662, loss: 0.17360903322696686\n",
      "step: 663, loss: 0.08560804277658463\n",
      "step: 664, loss: 0.15962837636470795\n",
      "step: 665, loss: 0.3455228805541992\n",
      "step: 666, loss: 0.4596192538738251\n",
      "step: 667, loss: 0.34892603754997253\n",
      "step: 668, loss: 0.1708964705467224\n",
      "step: 669, loss: 0.09114810079336166\n",
      "step: 670, loss: 0.16346114873886108\n",
      "step: 671, loss: 0.4140988886356354\n",
      "step: 672, loss: 0.07963334769010544\n",
      "step: 673, loss: 0.24215182662010193\n",
      "step: 674, loss: 0.33968213200569153\n",
      "step: 675, loss: 0.2704973816871643\n",
      "step: 676, loss: 0.0657806321978569\n",
      "step: 677, loss: 0.15811729431152344\n",
      "step: 678, loss: 0.06553936004638672\n",
      "step: 679, loss: 0.0213786531239748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 680, loss: 0.029345646500587463\n",
      "step: 681, loss: 0.08022693544626236\n",
      "step: 682, loss: 0.06573718786239624\n",
      "step: 683, loss: 0.011145596392452717\n",
      "step: 684, loss: 0.08723705261945724\n",
      "step: 685, loss: 0.039333585649728775\n",
      "step: 686, loss: 0.04381227120757103\n",
      "step: 687, loss: 0.01012243889272213\n",
      "step: 688, loss: 0.010051540099084377\n",
      "step: 689, loss: 0.020644424483180046\n",
      "step: 690, loss: 0.032692596316337585\n",
      "step: 691, loss: 0.009752648882567883\n",
      "step: 692, loss: 0.03246708959341049\n",
      "step: 693, loss: 0.06193813309073448\n",
      "step: 694, loss: 0.02907845564186573\n",
      "step: 695, loss: 0.0392715185880661\n",
      "step: 696, loss: 0.006971640978008509\n",
      "step: 697, loss: 0.03165365383028984\n",
      "step: 698, loss: 0.017996078357100487\n",
      "step: 699, loss: 0.015065351501107216\n",
      "step: 700, loss: 0.009882457554340363\n",
      "step: 701, loss: 0.011308488436043262\n",
      "step: 702, loss: 0.009653972461819649\n",
      "step: 703, loss: 0.020215604454278946\n",
      "step: 704, loss: 0.01863441988825798\n",
      "step: 705, loss: 0.09620153158903122\n",
      "step: 706, loss: 0.07668514549732208\n",
      "step: 707, loss: 0.027291269972920418\n",
      "step: 708, loss: 0.023875145241618156\n",
      "step: 709, loss: 0.0050034914165735245\n",
      "step: 710, loss: 0.11714310944080353\n",
      "step: 711, loss: 0.007841379381716251\n",
      "step: 712, loss: 0.05372663959860802\n",
      "step: 713, loss: 0.08020910620689392\n",
      "step: 714, loss: 0.03130565583705902\n",
      "step: 715, loss: 0.01124105416238308\n",
      "step: 716, loss: 0.040764905512332916\n",
      "step: 717, loss: 0.11107905209064484\n",
      "step: 718, loss: 0.13346007466316223\n",
      "step: 719, loss: 0.06035761535167694\n",
      "step: 720, loss: 0.025720156729221344\n",
      "step: 721, loss: 0.05285229533910751\n",
      "step: 722, loss: 0.04024161770939827\n",
      "step: 723, loss: 0.023074055090546608\n",
      "step: 724, loss: 0.05761837959289551\n",
      "step: 725, loss: 0.03429039567708969\n",
      "step: 726, loss: 0.016755495220422745\n",
      "step: 727, loss: 0.24861302971839905\n",
      "step: 728, loss: 0.014682332053780556\n",
      "step: 729, loss: 0.07682666927576065\n",
      "step: 730, loss: 0.003003246383741498\n",
      "step: 731, loss: 0.12944155931472778\n",
      "step: 732, loss: 0.019548635929822922\n",
      "step: 733, loss: 0.007730333600193262\n",
      "step: 734, loss: 0.03694019094109535\n",
      "step: 735, loss: 0.04304365813732147\n",
      "step: 736, loss: 0.07681065797805786\n",
      "step: 737, loss: 0.06483259052038193\n",
      "step: 738, loss: 0.12031841278076172\n",
      "step: 739, loss: 0.035122837871313095\n",
      "step: 740, loss: 0.04004891216754913\n",
      "step: 741, loss: 0.0068682595156133175\n",
      "step: 742, loss: 0.014825982041656971\n",
      "step: 743, loss: 0.03063955530524254\n",
      "step: 744, loss: 0.0186568945646286\n",
      "step: 745, loss: 0.09350612014532089\n",
      "step: 746, loss: 0.004660770762711763\n",
      "step: 747, loss: 0.06674961000680923\n",
      "step: 748, loss: 0.02590242587029934\n",
      "step: 749, loss: 0.010589320212602615\n",
      "step: 750, loss: 0.011269725859165192\n",
      "step: 751, loss: 0.04554016888141632\n",
      "step: 752, loss: 0.064799465239048\n",
      "step: 753, loss: 0.20054443180561066\n",
      "step: 754, loss: 0.17742305994033813\n",
      "step: 755, loss: 0.03934542089700699\n",
      "step: 756, loss: 0.016988184303045273\n",
      "step: 757, loss: 0.16037186980247498\n",
      "step: 758, loss: 0.13848398625850677\n",
      "step: 759, loss: 0.009378787130117416\n",
      "step: 760, loss: 0.005727394018322229\n",
      "step: 761, loss: 0.13241299986839294\n",
      "step: 762, loss: 0.07687227427959442\n",
      "step: 763, loss: 0.05872911959886551\n",
      "step: 764, loss: 0.028438633307814598\n",
      "step: 765, loss: 0.19425982236862183\n",
      "step: 766, loss: 0.005814440082758665\n",
      "step: 767, loss: 0.006335773505270481\n",
      "step: 768, loss: 0.010962342843413353\n",
      "step: 769, loss: 0.02873903512954712\n",
      "step: 770, loss: 0.06073682755231857\n",
      "step: 771, loss: 0.013808190822601318\n",
      "step: 772, loss: 0.026280783116817474\n",
      "step: 773, loss: 0.09514283388853073\n",
      "step: 774, loss: 0.04843587055802345\n",
      "step: 775, loss: 0.040389638394117355\n",
      "step: 776, loss: 0.03478892892599106\n",
      "step: 777, loss: 0.05902521312236786\n",
      "step: 778, loss: 0.04513648897409439\n",
      "step: 779, loss: 0.05691959336400032\n",
      "step: 780, loss: 0.01737838424742222\n",
      "step: 781, loss: 0.08337646722793579\n",
      "step: 782, loss: 0.04595379903912544\n",
      "step: 783, loss: 0.110348179936409\n",
      "step: 784, loss: 0.005921998992562294\n",
      "step: 785, loss: 0.007656586356461048\n",
      "step: 786, loss: 0.008201357908546925\n",
      "step: 787, loss: 0.03520623967051506\n",
      "step: 788, loss: 0.08475341647863388\n",
      "step: 789, loss: 0.09086504578590393\n",
      "step: 790, loss: 0.07670216262340546\n",
      "step: 791, loss: 0.018021974712610245\n",
      "step: 792, loss: 0.27571213245391846\n",
      "step: 793, loss: 0.07581772655248642\n",
      "step: 794, loss: 0.07184210419654846\n",
      "step: 795, loss: 0.025728454813361168\n",
      "step: 796, loss: 0.01785665936768055\n",
      "step: 797, loss: 0.038055457174777985\n",
      "step: 798, loss: 0.03657116740942001\n",
      "step: 799, loss: 0.10675746947526932\n",
      "step: 800, loss: 0.027564123272895813\n",
      "step: 801, loss: 0.019347338005900383\n",
      "step: 802, loss: 0.10947809368371964\n",
      "step: 803, loss: 0.11835697293281555\n",
      "step: 804, loss: 0.027844183146953583\n",
      "step: 805, loss: 0.016441309824585915\n",
      "step: 806, loss: 0.07299943268299103\n",
      "step: 807, loss: 0.08253584057092667\n",
      "step: 808, loss: 0.026581622660160065\n",
      "step: 809, loss: 0.00733255734667182\n",
      "step: 810, loss: 0.016483405604958534\n",
      "step: 811, loss: 0.007525216322392225\n",
      "step: 812, loss: 0.1092003658413887\n",
      "step: 813, loss: 0.21023885905742645\n",
      "step: 814, loss: 0.022668389603495598\n",
      "step: 815, loss: 0.03693598881363869\n",
      "step: 816, loss: 0.013045793399214745\n",
      "step: 817, loss: 0.05025642737746239\n",
      "step: 818, loss: 0.0024573258124291897\n",
      "step: 819, loss: 0.036030255258083344\n",
      "step: 820, loss: 0.026570433750748634\n",
      "step: 821, loss: 0.07540789991617203\n",
      "step: 822, loss: 0.004450983367860317\n",
      "step: 823, loss: 0.026731539517641068\n",
      "step: 824, loss: 0.010274816304445267\n",
      "step: 825, loss: 0.008917853236198425\n",
      "step: 826, loss: 0.05783656984567642\n",
      "step: 827, loss: 0.09166055917739868\n",
      "step: 828, loss: 0.009642689488828182\n",
      "step: 829, loss: 0.05439053475856781\n",
      "step: 830, loss: 0.046471599489450455\n",
      "step: 831, loss: 0.05990904942154884\n",
      "step: 832, loss: 0.07894577831029892\n",
      "step: 833, loss: 0.02262386865913868\n",
      "step: 834, loss: 0.012045311741530895\n",
      "step: 835, loss: 0.028322158381342888\n",
      "step: 836, loss: 0.03444409742951393\n",
      "step: 837, loss: 0.03286917507648468\n",
      "step: 838, loss: 0.003214541357010603\n",
      "step: 839, loss: 0.01912616938352585\n",
      "step: 840, loss: 0.013655237853527069\n",
      "step: 841, loss: 0.0059824599884450436\n",
      "step: 842, loss: 0.02465217188000679\n",
      "step: 843, loss: 0.06420829892158508\n",
      "step: 844, loss: 0.13315367698669434\n",
      "step: 845, loss: 0.04811547324061394\n",
      "step: 846, loss: 0.11992203444242477\n",
      "step: 847, loss: 0.14799876511096954\n",
      "step: 848, loss: 0.045550454407930374\n",
      "step: 849, loss: 0.09725505858659744\n",
      "step: 850, loss: 0.11184771358966827\n",
      "step: 851, loss: 0.01871289312839508\n",
      "step: 852, loss: 0.0055889226496219635\n",
      "step: 853, loss: 0.019821807742118835\n",
      "step: 854, loss: 0.04043509066104889\n",
      "step: 855, loss: 0.0295634213835001\n",
      "step: 856, loss: 0.0940793827176094\n",
      "step: 857, loss: 0.09791561216115952\n",
      "step: 858, loss: 0.010440107434988022\n",
      "step: 859, loss: 0.00467321090400219\n",
      "step: 860, loss: 0.022406723350286484\n",
      "step: 861, loss: 0.05959365889430046\n",
      "step: 862, loss: 0.021697863936424255\n",
      "step: 863, loss: 0.027611197903752327\n",
      "step: 864, loss: 0.009217971004545689\n",
      "step: 865, loss: 0.0947035625576973\n",
      "step: 866, loss: 0.021970070898532867\n",
      "step: 867, loss: 0.059956684708595276\n",
      "step: 868, loss: 0.057925473898649216\n",
      "step: 869, loss: 0.0049980864860117435\n",
      "step: 870, loss: 0.002842589048668742\n",
      "step: 871, loss: 0.00348039367236197\n",
      "step: 872, loss: 0.026912886649370193\n",
      "step: 873, loss: 0.03496577963232994\n",
      "step: 874, loss: 0.044743966311216354\n",
      "step: 875, loss: 0.09304182231426239\n",
      "step: 876, loss: 0.00363645376637578\n",
      "step: 877, loss: 0.0465724915266037\n",
      "step: 878, loss: 0.03136999160051346\n",
      "step: 879, loss: 0.006249193102121353\n",
      "step: 880, loss: 0.019576361402869225\n",
      "step: 881, loss: 0.0035156456287950277\n",
      "step: 882, loss: 0.12924368679523468\n",
      "step: 883, loss: 0.005990657955408096\n",
      "step: 884, loss: 0.0046393959783017635\n",
      "step: 885, loss: 0.01120708603411913\n",
      "step: 886, loss: 0.0033146238420158625\n",
      "step: 887, loss: 0.09355592727661133\n",
      "step: 888, loss: 0.04452279955148697\n",
      "step: 889, loss: 0.03659364581108093\n",
      "step: 890, loss: 0.016522789373993874\n",
      "step: 891, loss: 0.01452457346022129\n",
      "step: 892, loss: 0.060585737228393555\n",
      "step: 893, loss: 0.01760970428586006\n",
      "step: 894, loss: 0.025994978845119476\n",
      "step: 895, loss: 0.005512535572052002\n",
      "step: 896, loss: 0.03725922107696533\n",
      "step: 897, loss: 0.06369924545288086\n",
      "step: 898, loss: 0.09061601758003235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 899, loss: 0.042884450405836105\n",
      "step: 900, loss: 0.17204569280147552\n",
      "step: 901, loss: 0.008381057530641556\n",
      "step: 902, loss: 0.005290484055876732\n",
      "step: 903, loss: 0.02660476602613926\n",
      "step: 904, loss: 0.009726817719638348\n",
      "step: 905, loss: 0.00254641892388463\n",
      "step: 906, loss: 0.0033882802817970514\n",
      "step: 907, loss: 0.0489305704832077\n",
      "step: 908, loss: 0.013551464304327965\n",
      "step: 909, loss: 0.01788674108684063\n",
      "step: 910, loss: 0.0024142165202647448\n",
      "step: 911, loss: 0.0039303540252149105\n",
      "step: 912, loss: 0.008847295306622982\n",
      "step: 913, loss: 0.037087880074977875\n",
      "step: 914, loss: 0.005343422293663025\n",
      "step: 915, loss: 0.019573910161852837\n",
      "step: 916, loss: 0.0045377411879599094\n",
      "step: 917, loss: 0.008019454777240753\n",
      "step: 918, loss: 0.001844325684942305\n",
      "step: 919, loss: 0.004656457807868719\n",
      "step: 920, loss: 0.002317463979125023\n",
      "step: 921, loss: 0.004444102756679058\n",
      "step: 922, loss: 0.11221801489591599\n",
      "step: 923, loss: 0.003538410644978285\n",
      "step: 924, loss: 0.012986714020371437\n",
      "step: 925, loss: 0.023217424750328064\n",
      "step: 926, loss: 0.03179626539349556\n",
      "step: 927, loss: 0.008750307373702526\n",
      "step: 928, loss: 0.009248729795217514\n",
      "step: 929, loss: 0.008386493660509586\n",
      "step: 930, loss: 0.0034590743016451597\n",
      "step: 931, loss: 0.0018801104743033648\n",
      "step: 932, loss: 0.00351521885022521\n",
      "step: 933, loss: 0.07667239010334015\n",
      "step: 934, loss: 0.003737972117960453\n",
      "step: 935, loss: 0.0042394232004880905\n",
      "step: 936, loss: 0.00293001183308661\n",
      "step: 937, loss: 0.012905627489089966\n",
      "step: 938, loss: 0.007458065636456013\n",
      "step: 939, loss: 0.0030773156322538853\n",
      "step: 940, loss: 0.028716962784528732\n",
      "step: 941, loss: 0.1535922735929489\n",
      "step: 942, loss: 0.06728664040565491\n",
      "step: 943, loss: 0.049184780567884445\n",
      "step: 944, loss: 0.0040730200707912445\n",
      "step: 945, loss: 0.012410602532327175\n",
      "step: 946, loss: 0.02387002669274807\n",
      "step: 947, loss: 0.002899808343499899\n",
      "step: 948, loss: 0.06398539245128632\n",
      "step: 949, loss: 0.003955376334488392\n",
      "step: 950, loss: 0.0033446550369262695\n",
      "step: 951, loss: 0.0035567707382142544\n",
      "step: 952, loss: 0.00948270969092846\n",
      "step: 953, loss: 0.05561268702149391\n",
      "step: 954, loss: 0.017096012830734253\n",
      "step: 955, loss: 0.002163252327591181\n",
      "step: 956, loss: 0.08556883782148361\n",
      "step: 957, loss: 0.0017772684805095196\n",
      "step: 958, loss: 0.0036561847664415836\n",
      "step: 959, loss: 0.0054148118942976\n",
      "step: 960, loss: 0.006564924493432045\n",
      "step: 961, loss: 0.005766089074313641\n",
      "step: 962, loss: 0.003536646254360676\n",
      "step: 963, loss: 0.0034514686558395624\n",
      "step: 964, loss: 0.0024341160897165537\n",
      "step: 965, loss: 0.006824138108640909\n",
      "step: 966, loss: 0.14154554903507233\n",
      "step: 967, loss: 0.012984598986804485\n",
      "step: 968, loss: 0.0032175725791603327\n",
      "step: 969, loss: 0.008077703416347504\n",
      "step: 970, loss: 0.002202705480158329\n",
      "step: 971, loss: 0.015949390828609467\n",
      "step: 972, loss: 0.050710149109363556\n",
      "step: 973, loss: 0.02845241315662861\n",
      "step: 974, loss: 0.0011195219121873379\n",
      "step: 975, loss: 0.0032525924034416676\n",
      "step: 976, loss: 0.0019738588016480207\n",
      "step: 977, loss: 0.016047725453972816\n",
      "step: 978, loss: 0.07249408215284348\n",
      "step: 979, loss: 0.008775684051215649\n",
      "step: 980, loss: 0.03062218427658081\n",
      "step: 981, loss: 0.005447257775813341\n",
      "step: 982, loss: 0.012254392728209496\n",
      "step: 983, loss: 0.004683827981352806\n",
      "step: 984, loss: 0.0015338337980210781\n",
      "step: 985, loss: 0.004798179492354393\n",
      "step: 986, loss: 0.022419951856136322\n",
      "step: 987, loss: 0.007767336908727884\n",
      "step: 988, loss: 0.0026444406248629093\n",
      "step: 989, loss: 0.008778453804552555\n",
      "step: 990, loss: 0.018874507397413254\n",
      "step: 991, loss: 0.001701186178252101\n",
      "step: 992, loss: 0.003707525786012411\n",
      "step: 993, loss: 0.011533460579812527\n",
      "step: 994, loss: 0.001936047920025885\n",
      "step: 995, loss: 0.007594147231429815\n",
      "step: 996, loss: 0.009119664318859577\n",
      "step: 997, loss: 0.007910946384072304\n",
      "step: 998, loss: 0.0025098887272179127\n",
      "step: 999, loss: 0.0040411073714494705\n",
      "step: 1000, loss: 0.06221809983253479\n",
      "step: 1001, loss: 0.007905674166977406\n",
      "step: 1002, loss: 0.005729806609451771\n",
      "step: 1003, loss: 0.002345578745007515\n",
      "step: 1004, loss: 0.013173610903322697\n",
      "step: 1005, loss: 0.003092816798016429\n",
      "step: 1006, loss: 0.027168355882167816\n",
      "step: 1007, loss: 0.001899244962260127\n",
      "step: 1008, loss: 0.04523121938109398\n",
      "step: 1009, loss: 0.1251940131187439\n",
      "step: 1010, loss: 0.008050942793488503\n",
      "step: 1011, loss: 0.005641012918204069\n",
      "step: 1012, loss: 0.033867910504341125\n",
      "step: 1013, loss: 0.003882952034473419\n",
      "step: 1014, loss: 0.013116610236465931\n",
      "step: 1015, loss: 0.06875469535589218\n",
      "step: 1016, loss: 0.013782776892185211\n",
      "step: 1017, loss: 0.0038575977087020874\n",
      "step: 1018, loss: 0.05110423266887665\n",
      "step: 1019, loss: 0.002354949014261365\n",
      "step: 1020, loss: 0.0053114271722733974\n",
      "step: 1021, loss: 0.005492443218827248\n",
      "step: 1022, loss: 0.015121806412935257\n",
      "step: 1023, loss: 0.0024129704106599092\n",
      "step: 1024, loss: 0.01483957190066576\n",
      "step: 1025, loss: 0.002521368907764554\n",
      "step: 1026, loss: 0.010105418972671032\n",
      "step: 1027, loss: 0.03999323025345802\n",
      "step: 1028, loss: 0.018312959000468254\n",
      "step: 1029, loss: 0.004051629453897476\n",
      "step: 1030, loss: 0.027830062434077263\n",
      "step: 1031, loss: 0.00552642485126853\n",
      "step: 1032, loss: 0.002925300970673561\n",
      "step: 1033, loss: 0.0033322819508612156\n",
      "step: 1034, loss: 0.0013941049110144377\n",
      "step: 1035, loss: 0.004729418084025383\n",
      "step: 1036, loss: 0.055040087550878525\n",
      "step: 1037, loss: 0.02976430393755436\n",
      "step: 1038, loss: 0.029219843447208405\n",
      "step: 1039, loss: 0.003522171638906002\n",
      "step: 1040, loss: 0.003266368992626667\n",
      "step: 1041, loss: 0.0025756857357919216\n",
      "step: 1042, loss: 0.011499459855258465\n",
      "step: 1043, loss: 0.005533511750400066\n",
      "step: 1044, loss: 0.0005745170637965202\n",
      "step: 1045, loss: 0.05395307019352913\n",
      "step: 1046, loss: 0.023661816492676735\n",
      "step: 1047, loss: 0.001449134899303317\n",
      "step: 1048, loss: 0.004303930792957544\n",
      "step: 1049, loss: 0.0018784503918141127\n",
      "step: 1050, loss: 0.007001487072557211\n",
      "step: 1051, loss: 0.003883414901793003\n",
      "step: 1052, loss: 0.0031627104617655277\n",
      "step: 1053, loss: 0.2607805132865906\n",
      "step: 1054, loss: 0.005842289887368679\n",
      "step: 1055, loss: 0.009099307470023632\n",
      "step: 1056, loss: 0.013333219103515148\n",
      "step: 1057, loss: 0.009472851641476154\n",
      "step: 1058, loss: 0.001580933341756463\n",
      "step: 1059, loss: 0.013160777278244495\n",
      "step: 1060, loss: 0.02650216594338417\n",
      "step: 1061, loss: 0.005856641102582216\n",
      "step: 1062, loss: 0.005970722064375877\n",
      "step: 1063, loss: 0.009395129978656769\n",
      "step: 1064, loss: 0.0017730261897668242\n",
      "step: 1065, loss: 0.013807999901473522\n",
      "step: 1066, loss: 0.002694991184398532\n",
      "step: 1067, loss: 0.0029002258088439703\n",
      "step: 1068, loss: 0.1801522672176361\n",
      "step: 1069, loss: 0.03970691189169884\n",
      "step: 1070, loss: 0.011969383805990219\n",
      "step: 1071, loss: 0.0663318932056427\n",
      "step: 1072, loss: 0.0008109802147373557\n",
      "step: 1073, loss: 0.005176595877856016\n",
      "step: 1074, loss: 0.005755850113928318\n",
      "step: 1075, loss: 0.0008740488556213677\n",
      "step: 1076, loss: 0.012386244721710682\n",
      "step: 1077, loss: 0.009821743704378605\n",
      "step: 1078, loss: 0.001046086079441011\n",
      "step: 1079, loss: 0.010877003893256187\n",
      "step: 1080, loss: 0.003781945211812854\n",
      "step: 1081, loss: 0.013316567055881023\n",
      "step: 1082, loss: 0.0029523735865950584\n",
      "step: 1083, loss: 0.018303269520401955\n",
      "step: 1084, loss: 0.09654958546161652\n",
      "step: 1085, loss: 0.0014677818398922682\n",
      "step: 1086, loss: 0.009386259131133556\n",
      "step: 1087, loss: 0.009248130023479462\n",
      "step: 1088, loss: 0.003992096986621618\n",
      "step: 1089, loss: 0.004547324031591415\n",
      "step: 1090, loss: 0.026113348081707954\n",
      "step: 1091, loss: 0.031610164791345596\n",
      "step: 1092, loss: 0.004804832860827446\n",
      "step: 1093, loss: 0.06205633282661438\n",
      "step: 1094, loss: 0.0021925438195466995\n",
      "step: 1095, loss: 0.0014905462739989161\n",
      "step: 1096, loss: 0.0014683245681226254\n",
      "step: 1097, loss: 0.002401083242148161\n",
      "step: 1098, loss: 0.003278380027040839\n",
      "step: 1099, loss: 0.0019873694982379675\n",
      "step: 1100, loss: 0.010140946134924889\n",
      "step: 1101, loss: 0.07002007216215134\n",
      "step: 1102, loss: 0.003584463382139802\n",
      "step: 1103, loss: 0.004031313117593527\n",
      "step: 1104, loss: 0.0037488110829144716\n",
      "step: 1105, loss: 0.018605070188641548\n",
      "step: 1106, loss: 0.021722884848713875\n",
      "step: 1107, loss: 0.003539645578712225\n",
      "step: 1108, loss: 0.0014783379156142473\n",
      "step: 1109, loss: 0.0022405830677598715\n",
      "step: 1110, loss: 0.002281680004671216\n",
      "step: 1111, loss: 0.004779244773089886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1112, loss: 0.0200326070189476\n",
      "step: 1113, loss: 0.001385542913340032\n",
      "step: 1114, loss: 0.02080102451145649\n",
      "step: 1115, loss: 0.0017004951369017363\n",
      "step: 1116, loss: 0.0023973321076482534\n",
      "step: 1117, loss: 0.030768992379307747\n",
      "step: 1118, loss: 0.002754118526354432\n",
      "step: 1119, loss: 0.03287778049707413\n",
      "step: 1120, loss: 0.017438707873225212\n",
      "step: 1121, loss: 0.00553040811792016\n",
      "step: 1122, loss: 0.03655075281858444\n",
      "step: 1123, loss: 0.02441086806356907\n",
      "step: 1124, loss: 0.02294185198843479\n",
      "step: 1125, loss: 0.0016561629017814994\n",
      "step: 1126, loss: 0.0025229318998754025\n",
      "step: 1127, loss: 0.0027365682180970907\n",
      "step: 1128, loss: 0.0015428499318659306\n",
      "step: 1129, loss: 0.06559186428785324\n",
      "step: 1130, loss: 0.0034861706662923098\n",
      "step: 1131, loss: 0.0017665892373770475\n",
      "step: 1132, loss: 0.0022278290707618\n",
      "step: 1133, loss: 0.001054846215993166\n",
      "step: 1134, loss: 0.03508150950074196\n",
      "step: 1135, loss: 0.13068729639053345\n",
      "step: 1136, loss: 0.03173399344086647\n",
      "step: 1137, loss: 0.06519997864961624\n",
      "step: 1138, loss: 0.014056934043765068\n",
      "step: 1139, loss: 0.0007702711154706776\n",
      "step: 1140, loss: 0.0014296701410785317\n",
      "step: 1141, loss: 0.002778167137876153\n",
      "step: 1142, loss: 0.001732317847199738\n",
      "step: 1143, loss: 0.0017792225116863847\n",
      "step: 1144, loss: 0.0022137134801596403\n",
      "step: 1145, loss: 0.14694546163082123\n",
      "step: 1146, loss: 0.0017786807147786021\n",
      "step: 1147, loss: 0.0012722761603072286\n",
      "step: 1148, loss: 0.0016555461334064603\n",
      "step: 1149, loss: 0.0015482323942705989\n",
      "step: 1150, loss: 0.05679138004779816\n",
      "step: 1151, loss: 0.0019271119963377714\n",
      "step: 1152, loss: 0.0014557611430063844\n",
      "step: 1153, loss: 0.0022044747602194548\n",
      "step: 1154, loss: 0.01225947868078947\n",
      "step: 1155, loss: 0.05719969421625137\n",
      "step: 1156, loss: 0.0009201852953992784\n",
      "step: 1157, loss: 0.017513001337647438\n",
      "step: 1158, loss: 0.005605020094662905\n",
      "step: 1159, loss: 0.0003930477541871369\n",
      "step: 1160, loss: 0.0027635658625513315\n",
      "step: 1161, loss: 0.004988640546798706\n",
      "step: 1162, loss: 0.03022150509059429\n",
      "step: 1163, loss: 0.002577801002189517\n",
      "step: 1164, loss: 0.04687677323818207\n",
      "step: 1165, loss: 0.001423062989488244\n",
      "step: 1166, loss: 0.0008708974928595126\n",
      "step: 1167, loss: 0.0016471057897433639\n",
      "step: 1168, loss: 0.001376891159452498\n",
      "step: 1169, loss: 0.004285501316189766\n",
      "step: 1170, loss: 0.0010179529199376702\n",
      "step: 1171, loss: 0.0027771133463829756\n",
      "step: 1172, loss: 0.0013067456893622875\n",
      "step: 1173, loss: 0.0018134404672309756\n",
      "step: 1174, loss: 0.0018166176741942763\n",
      "step: 1175, loss: 0.00431384751573205\n",
      "step: 1176, loss: 0.002615935867652297\n",
      "step: 1177, loss: 0.0023892682511359453\n",
      "step: 1178, loss: 0.0027904105372726917\n",
      "step: 1179, loss: 0.0022839673329144716\n",
      "step: 1180, loss: 0.013438008725643158\n",
      "step: 1181, loss: 0.00162549689412117\n",
      "step: 1182, loss: 0.0019460137700662017\n",
      "step: 1183, loss: 0.004286025185137987\n",
      "step: 1184, loss: 0.008387397974729538\n",
      "step: 1185, loss: 0.0024195772130042315\n",
      "step: 1186, loss: 0.001839147531427443\n",
      "step: 1187, loss: 0.0018948103534057736\n",
      "step: 1188, loss: 0.0009261832456104457\n",
      "step: 1189, loss: 0.0024266631808131933\n",
      "step: 1190, loss: 0.0009342662524431944\n",
      "step: 1191, loss: 0.025336256250739098\n",
      "step: 1192, loss: 0.0025396381970494986\n",
      "step: 1193, loss: 0.0016724714078009129\n",
      "step: 1194, loss: 0.0018609617836773396\n",
      "step: 1195, loss: 0.0012426349567249417\n",
      "step: 1196, loss: 0.004003190901130438\n",
      "step: 1197, loss: 0.0012781238183379173\n",
      "step: 1198, loss: 0.008104399777948856\n",
      "step: 1199, loss: 0.0011012452887371182\n",
      "step: 1200, loss: 0.0014104360016062856\n",
      "step: 1201, loss: 0.0069831861183047295\n",
      "step: 1202, loss: 0.003472336567938328\n",
      "step: 1203, loss: 0.003611515276134014\n",
      "step: 1204, loss: 0.007758308667689562\n",
      "step: 1205, loss: 0.0010984805412590504\n",
      "step: 1206, loss: 0.002359947422519326\n",
      "step: 1207, loss: 0.000643967418000102\n",
      "step: 1208, loss: 0.001044288626872003\n",
      "step: 1209, loss: 0.06814207881689072\n",
      "step: 1210, loss: 0.0013160035014152527\n",
      "step: 1211, loss: 0.003292662091553211\n",
      "step: 1212, loss: 0.0012516240822151303\n",
      "step: 1213, loss: 0.0013076582690700889\n",
      "step: 1214, loss: 0.0015838807448744774\n",
      "step: 1215, loss: 0.05138605460524559\n",
      "step: 1216, loss: 0.0005992520600557327\n",
      "step: 1217, loss: 0.02039359137415886\n",
      "step: 1218, loss: 0.0012896661646664143\n",
      "step: 1219, loss: 0.0015820390544831753\n",
      "step: 1220, loss: 0.0009782868437469006\n",
      "step: 1221, loss: 0.033388253301382065\n",
      "step: 1222, loss: 0.0010639246320351958\n",
      "step: 1223, loss: 0.0020200074650347233\n",
      "step: 1224, loss: 0.0013965953839942813\n",
      "step: 1225, loss: 0.00820754561573267\n",
      "step: 1226, loss: 0.0011157997651025653\n",
      "step: 1227, loss: 0.001293177017942071\n",
      "step: 1228, loss: 0.010693442076444626\n",
      "step: 1229, loss: 0.0024048478808254004\n",
      "step: 1230, loss: 0.0009229157003574073\n",
      "step: 1231, loss: 0.0027363405097275972\n",
      "step: 1232, loss: 0.035950589925050735\n",
      "step: 1233, loss: 0.0012408230686560273\n",
      "step: 1234, loss: 0.0024139233864843845\n",
      "step: 1235, loss: 0.0019846996292471886\n",
      "step: 1236, loss: 0.0008889376185834408\n",
      "step: 1237, loss: 0.05842055752873421\n",
      "step: 1238, loss: 0.003947162535041571\n",
      "step: 1239, loss: 0.005169427953660488\n",
      "step: 1240, loss: 0.01708979904651642\n",
      "step: 1241, loss: 0.038050100207328796\n",
      "step: 1242, loss: 0.009605847299098969\n",
      "step: 1243, loss: 0.001561817480251193\n",
      "step: 1244, loss: 0.0032774677965790033\n",
      "step: 1245, loss: 0.001521824742667377\n",
      "step: 1246, loss: 0.0015753727639093995\n",
      "step: 1247, loss: 0.0019747756887227297\n",
      "step: 1248, loss: 0.0007590250461362302\n",
      "step: 1249, loss: 0.003351788269355893\n",
      "step: 1250, loss: 0.0015324512496590614\n",
      "step: 1251, loss: 0.0012495293049141765\n",
      "step: 1252, loss: 0.0034532411955296993\n",
      "step: 1253, loss: 0.008421588689088821\n",
      "step: 1254, loss: 0.000828720920253545\n",
      "step: 1255, loss: 0.0007131193415261805\n",
      "step: 1256, loss: 0.0010809425730258226\n",
      "step: 1257, loss: 0.015741771087050438\n",
      "step: 1258, loss: 0.0012496468843892217\n",
      "step: 1259, loss: 0.00155816285405308\n",
      "step: 1260, loss: 0.001302634016610682\n",
      "step: 1261, loss: 0.0011983965523540974\n",
      "step: 1262, loss: 0.0010490125278010964\n",
      "step: 1263, loss: 0.0024890757631510496\n",
      "step: 1264, loss: 0.0007328997598960996\n",
      "step: 1265, loss: 0.0008148204651661217\n",
      "step: 1266, loss: 0.001656338688917458\n",
      "step: 1267, loss: 0.0011961866402998567\n",
      "step: 1268, loss: 0.0017674448899924755\n",
      "step: 1269, loss: 0.0010752934031188488\n",
      "step: 1270, loss: 0.0016483734361827374\n",
      "step: 1271, loss: 0.0010536175686866045\n",
      "step: 1272, loss: 0.09093965590000153\n",
      "step: 1273, loss: 0.002761570503935218\n",
      "step: 1274, loss: 0.0010831053368747234\n",
      "step: 1275, loss: 0.0034655951894819736\n",
      "step: 1276, loss: 0.007292283698916435\n",
      "step: 1277, loss: 0.0012222669320181012\n",
      "step: 1278, loss: 0.0010308165801689029\n",
      "step: 1279, loss: 0.02512245438992977\n",
      "step: 1280, loss: 0.0071699912659823895\n",
      "step: 1281, loss: 0.03218896687030792\n",
      "step: 1282, loss: 0.008869302459061146\n",
      "step: 1283, loss: 0.001053177285939455\n",
      "step: 1284, loss: 0.0017838295316323638\n",
      "step: 1285, loss: 0.07010408490896225\n",
      "step: 1286, loss: 0.019194595515727997\n",
      "step: 1287, loss: 0.0009546098299324512\n",
      "step: 1288, loss: 0.0006298249936662614\n",
      "step: 1289, loss: 0.0012899244902655482\n",
      "step: 1290, loss: 0.0008729135151952505\n",
      "step: 1291, loss: 0.0024326839484274387\n",
      "step: 1292, loss: 0.005194811150431633\n",
      "step: 1293, loss: 0.0009580115438438952\n",
      "step: 1294, loss: 0.0013654163340106606\n",
      "step: 1295, loss: 0.0015103108016774058\n",
      "step: 1296, loss: 0.001054504420608282\n",
      "step: 1297, loss: 0.002658618614077568\n",
      "step: 1298, loss: 0.007250400260090828\n",
      "step: 1299, loss: 0.002849099924787879\n",
      "step: 1300, loss: 0.016789820045232773\n",
      "step: 1301, loss: 0.0009304923587478697\n",
      "step: 1302, loss: 0.00042228016536682844\n",
      "step: 1303, loss: 0.0013579556252807379\n",
      "step: 1304, loss: 0.0050101513043046\n",
      "step: 1305, loss: 0.0017787301912903786\n",
      "step: 1306, loss: 0.004430173896253109\n",
      "step: 1307, loss: 0.0019472814165055752\n",
      "step: 1308, loss: 0.001903268857859075\n",
      "step: 1309, loss: 0.0009276329074054956\n",
      "step: 1310, loss: 0.012440484017133713\n",
      "step: 1311, loss: 0.0031524216756224632\n",
      "step: 1312, loss: 0.012176137417554855\n",
      "step: 1313, loss: 0.0032105189748108387\n",
      "step: 1314, loss: 0.003103140974417329\n",
      "step: 1315, loss: 0.0031476204749196768\n",
      "step: 1316, loss: 0.000570001604501158\n",
      "step: 1317, loss: 0.0005536684184335172\n",
      "step: 1318, loss: 0.0023237154819071293\n",
      "step: 1319, loss: 0.001445914153009653\n",
      "step: 1320, loss: 0.0011976384557783604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1321, loss: 0.001698166597634554\n",
      "step: 1322, loss: 0.0010615320643410087\n",
      "step: 1323, loss: 0.06976860761642456\n",
      "step: 1324, loss: 0.002934265648946166\n",
      "step: 1325, loss: 0.004334019031375647\n",
      "step: 1326, loss: 0.0029400617349892855\n",
      "step: 1327, loss: 0.0012644014786928892\n",
      "step: 1328, loss: 0.0006774350185878575\n",
      "step: 1329, loss: 0.0022243098355829716\n",
      "step: 1330, loss: 0.0032355489674955606\n",
      "step: 1331, loss: 0.0011286746012046933\n",
      "step: 1332, loss: 0.0008080317638814449\n",
      "step: 1333, loss: 0.0019507366232573986\n",
      "step: 1334, loss: 0.08593236654996872\n",
      "step: 1335, loss: 0.0006588103133253753\n",
      "step: 1336, loss: 0.07418236136436462\n",
      "step: 1337, loss: 0.0006174702430143952\n",
      "step: 1338, loss: 0.03533980995416641\n",
      "step: 1339, loss: 0.0013102262746542692\n",
      "step: 1340, loss: 0.000928698864299804\n",
      "step: 1341, loss: 0.0014840241055935621\n",
      "step: 1342, loss: 0.0011980077251791954\n",
      "step: 1343, loss: 0.011750426143407822\n",
      "step: 1344, loss: 0.019468899816274643\n",
      "step: 1345, loss: 0.0036622313782572746\n",
      "step: 1346, loss: 0.00505198584869504\n",
      "step: 1347, loss: 0.0010943448869511485\n",
      "step: 1348, loss: 0.0696144551038742\n",
      "step: 1349, loss: 0.0006717922515235841\n",
      "step: 1350, loss: 0.0026459386572241783\n",
      "step: 1351, loss: 0.001464207423850894\n",
      "step: 1352, loss: 0.002380123594775796\n",
      "step: 1353, loss: 0.002597267273813486\n",
      "step: 1354, loss: 0.0019117933697998524\n",
      "step: 1355, loss: 0.014515775255858898\n",
      "step: 1356, loss: 0.0005977715482003987\n",
      "step: 1357, loss: 0.0007376933936029673\n",
      "step: 1358, loss: 0.0008848616271279752\n",
      "step: 1359, loss: 0.002147112740203738\n",
      "step: 1360, loss: 0.0018248589476570487\n",
      "step: 1361, loss: 0.0014814967289566994\n",
      "step: 1362, loss: 0.000764822238124907\n",
      "step: 1363, loss: 0.0013145480770617723\n",
      "step: 1364, loss: 0.0013543831882998347\n",
      "step: 1365, loss: 0.027075931429862976\n",
      "step: 1366, loss: 0.0010476844618096948\n",
      "step: 1367, loss: 0.002078298944979906\n",
      "step: 1368, loss: 0.002982201986014843\n",
      "step: 1369, loss: 0.00039955813554115593\n",
      "step: 1370, loss: 0.0010131977032870054\n",
      "step: 1371, loss: 0.0011999328853562474\n",
      "step: 1372, loss: 0.024812985211610794\n",
      "step: 1373, loss: 0.0010088436538353562\n",
      "step: 1374, loss: 0.018940307199954987\n",
      "step: 1375, loss: 0.0011939051328226924\n",
      "step: 1376, loss: 0.0008956031524576247\n",
      "step: 1377, loss: 0.0011928125750273466\n",
      "step: 1378, loss: 0.0005663632764481008\n",
      "step: 1379, loss: 0.003983267117291689\n",
      "step: 1380, loss: 0.0015716355992481112\n",
      "step: 1381, loss: 0.0014145304448902607\n",
      "step: 1382, loss: 0.0011879001976922154\n",
      "step: 1383, loss: 0.0012501445598900318\n",
      "step: 1384, loss: 0.0008795267203822732\n",
      "step: 1385, loss: 0.0016096632461994886\n",
      "step: 1386, loss: 0.031118430197238922\n",
      "step: 1387, loss: 0.0008490490145049989\n",
      "step: 1388, loss: 0.005067951511591673\n",
      "step: 1389, loss: 0.015198434703052044\n",
      "step: 1390, loss: 0.0006019131978973746\n",
      "step: 1391, loss: 0.001199719263240695\n",
      "step: 1392, loss: 0.001690075732767582\n",
      "step: 1393, loss: 0.015439719893038273\n",
      "step: 1394, loss: 0.000711317581590265\n",
      "step: 1395, loss: 0.059548161923885345\n",
      "step: 1396, loss: 0.002628037706017494\n",
      "step: 1397, loss: 0.01402893103659153\n",
      "step: 1398, loss: 0.002943381667137146\n",
      "step: 1399, loss: 0.000720221025403589\n",
      "step: 1400, loss: 0.0012619000626727939\n",
      "step: 1401, loss: 0.008803851902484894\n",
      "step: 1402, loss: 0.0010590050369501114\n",
      "step: 1403, loss: 0.0005477728554978967\n",
      "step: 1404, loss: 0.052111320197582245\n",
      "step: 1405, loss: 0.002444860525429249\n",
      "step: 1406, loss: 0.013499518856406212\n",
      "step: 1407, loss: 0.0018271945882588625\n",
      "step: 1408, loss: 0.0007754180114716291\n",
      "step: 1409, loss: 0.0014840529765933752\n",
      "step: 1410, loss: 0.0005464612040668726\n",
      "step: 1411, loss: 0.005835387855768204\n",
      "step: 1412, loss: 0.0007003292557783425\n",
      "step: 1413, loss: 0.0015204489463940263\n",
      "step: 1414, loss: 0.0010593801271170378\n",
      "step: 1415, loss: 0.0006651153671555221\n",
      "step: 1416, loss: 0.0007432271377183497\n",
      "step: 1417, loss: 0.0017444636905565858\n",
      "step: 1418, loss: 0.00081756676081568\n",
      "step: 1419, loss: 0.0011953203938901424\n",
      "step: 1420, loss: 0.002829077187925577\n",
      "step: 1421, loss: 0.0012863570591434836\n",
      "step: 1422, loss: 0.00122304807882756\n",
      "step: 1423, loss: 0.0010256451787427068\n",
      "step: 1424, loss: 0.0015199952758848667\n",
      "step: 1425, loss: 0.004731886554509401\n",
      "step: 1426, loss: 0.0038713859394192696\n",
      "step: 1427, loss: 0.0006607953691855073\n",
      "step: 1428, loss: 0.0005589030915871263\n",
      "step: 1429, loss: 0.015109610743820667\n",
      "step: 1430, loss: 0.0016710921190679073\n",
      "step: 1431, loss: 0.003122287802398205\n",
      "step: 1432, loss: 0.001091725192964077\n",
      "step: 1433, loss: 0.0018532645190134645\n",
      "step: 1434, loss: 0.0013314138632267714\n",
      "step: 1435, loss: 0.0006003216840326786\n",
      "step: 1436, loss: 0.001550602726638317\n",
      "step: 1437, loss: 0.03595757484436035\n",
      "step: 1438, loss: 0.002152400789782405\n",
      "step: 1439, loss: 0.0012646194081753492\n",
      "step: 1440, loss: 0.0011874065967276692\n",
      "step: 1441, loss: 0.0005907004815526307\n",
      "step: 1442, loss: 0.001984283095225692\n",
      "step: 1443, loss: 0.0010419184109196067\n",
      "step: 1444, loss: 0.0007835563737899065\n",
      "step: 1445, loss: 0.0007366277277469635\n",
      "step: 1446, loss: 0.0004732646339107305\n",
      "step: 1447, loss: 0.000831276411190629\n",
      "step: 1448, loss: 0.0010558227077126503\n",
      "step: 1449, loss: 0.0013929866254329681\n",
      "step: 1450, loss: 0.0009413448860868812\n",
      "step: 1451, loss: 0.0005989352357573807\n",
      "step: 1452, loss: 0.027554946020245552\n",
      "step: 1453, loss: 0.0013645183062180877\n",
      "step: 1454, loss: 0.0013511963188648224\n",
      "step: 1455, loss: 0.0011471564648672938\n",
      "step: 1456, loss: 0.015457899309694767\n",
      "step: 1457, loss: 0.030609894543886185\n",
      "step: 1458, loss: 0.009649284183979034\n",
      "step: 1459, loss: 0.021250290796160698\n",
      "step: 1460, loss: 0.0691002905368805\n",
      "step: 1461, loss: 0.0009559227037243545\n",
      "step: 1462, loss: 0.0011540548875927925\n",
      "step: 1463, loss: 0.000718708906788379\n",
      "step: 1464, loss: 0.00036738498602062464\n",
      "step: 1465, loss: 0.00026382756186649203\n",
      "step: 1466, loss: 0.000681077188346535\n",
      "step: 1467, loss: 0.0007878565811552107\n",
      "step: 1468, loss: 0.00148774532135576\n",
      "step: 1469, loss: 0.0005607132334262133\n",
      "step: 1470, loss: 0.0015398528194054961\n",
      "step: 1471, loss: 0.001999993808567524\n",
      "step: 1472, loss: 0.019466020166873932\n",
      "step: 1473, loss: 0.0005708885728381574\n",
      "step: 1474, loss: 0.0010043734218925238\n",
      "step: 1475, loss: 0.0005323617369867861\n",
      "step: 1476, loss: 0.0007285571191459894\n",
      "step: 1477, loss: 0.004107913002371788\n",
      "step: 1478, loss: 0.0005207379581406713\n",
      "step: 1479, loss: 0.0006146184168756008\n",
      "step: 1480, loss: 0.00035658589331433177\n",
      "step: 1481, loss: 0.0006099941674619913\n",
      "step: 1482, loss: 0.000647303881123662\n",
      "step: 1483, loss: 0.0022364542819559574\n",
      "step: 1484, loss: 0.0010660411790013313\n",
      "step: 1485, loss: 0.0032621826976537704\n",
      "step: 1486, loss: 0.0009527758229523897\n",
      "step: 1487, loss: 0.0004330925294198096\n",
      "step: 1488, loss: 0.0018253051675856113\n",
      "step: 1489, loss: 0.003978610038757324\n",
      "step: 1490, loss: 0.001040574861690402\n",
      "step: 1491, loss: 0.0007923951488919556\n",
      "step: 1492, loss: 0.0008845867123454809\n",
      "step: 1493, loss: 0.0017748675309121609\n",
      "step: 1494, loss: 0.000672874681185931\n",
      "step: 1495, loss: 0.0014041971880942583\n",
      "step: 1496, loss: 0.0003262122627347708\n",
      "step: 1497, loss: 0.0006025771144777536\n",
      "step: 1498, loss: 0.0014332847204059362\n",
      "step: 1499, loss: 0.0005734405131079257\n",
      "step: 1500, loss: 0.0009782169945538044\n",
      "step: 1501, loss: 0.0002536255051381886\n",
      "step: 1502, loss: 0.0022634812630712986\n",
      "step: 1503, loss: 0.0005504550063051283\n",
      "step: 1504, loss: 0.0008472300251014531\n",
      "step: 1505, loss: 0.0026492809411138296\n",
      "step: 1506, loss: 0.00023228413192555308\n",
      "step: 1507, loss: 0.0009114213171415031\n",
      "step: 1508, loss: 0.0013003273634240031\n",
      "step: 1509, loss: 0.0003519884776324034\n",
      "step: 1510, loss: 0.0011126055615022779\n",
      "step: 1511, loss: 0.0009136769804172218\n",
      "step: 1512, loss: 0.0016007849480956793\n",
      "step: 1513, loss: 0.0007176476065069437\n",
      "step: 1514, loss: 0.0005446966970339417\n",
      "step: 1515, loss: 0.0008772113942541182\n",
      "step: 1516, loss: 0.054279595613479614\n",
      "step: 1517, loss: 0.0008850787417031825\n",
      "step: 1518, loss: 0.0005668874946422875\n",
      "step: 1519, loss: 0.0022310581989586353\n",
      "step: 1520, loss: 0.0011871192837134004\n",
      "step: 1521, loss: 0.017643161118030548\n",
      "step: 1522, loss: 0.0005902888369746506\n",
      "step: 1523, loss: 0.0037811966612935066\n",
      "step: 1524, loss: 0.0011361348442733288\n",
      "step: 1525, loss: 0.000837267201859504\n",
      "step: 1526, loss: 0.0005723339854739606\n",
      "step: 1527, loss: 0.0010012096026912332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1528, loss: 0.0010177125222980976\n",
      "step: 1529, loss: 0.0007720980793237686\n",
      "step: 1530, loss: 0.0025898509193211794\n",
      "step: 1531, loss: 0.0011940659023821354\n",
      "step: 1532, loss: 0.03564450517296791\n",
      "step: 1533, loss: 0.0015396528178825974\n",
      "step: 1534, loss: 0.0009013635572046041\n",
      "step: 1535, loss: 0.000566568342037499\n",
      "step: 1536, loss: 0.001389933517202735\n",
      "step: 1537, loss: 0.0004902931395918131\n",
      "step: 1538, loss: 0.0002609170915093273\n",
      "step: 1539, loss: 0.0025856492575258017\n",
      "step: 1540, loss: 0.001151951146312058\n",
      "step: 1541, loss: 0.020815562456846237\n",
      "step: 1542, loss: 0.0019288378534838557\n",
      "step: 1543, loss: 0.0008034247439354658\n",
      "step: 1544, loss: 0.001980260480195284\n",
      "step: 1545, loss: 0.0008401434170082211\n",
      "step: 1546, loss: 0.0009469232754781842\n",
      "step: 1547, loss: 0.001456920406781137\n",
      "step: 1548, loss: 0.007915521040558815\n",
      "step: 1549, loss: 0.031564727425575256\n",
      "step: 1550, loss: 0.0008058664388954639\n",
      "step: 1551, loss: 0.001272305496968329\n",
      "step: 1552, loss: 0.006085550878196955\n",
      "step: 1553, loss: 0.0004298364510759711\n",
      "step: 1554, loss: 0.0010292144725099206\n",
      "step: 1555, loss: 0.0027056944090873003\n",
      "step: 1556, loss: 0.0012077165301889181\n",
      "step: 1557, loss: 0.0006333303172141314\n",
      "step: 1558, loss: 0.016771478578448296\n",
      "step: 1559, loss: 0.000500901136547327\n",
      "step: 1560, loss: 0.000576361664570868\n",
      "step: 1561, loss: 0.000734674627892673\n",
      "step: 1562, loss: 0.0005941641866229475\n",
      "step: 1563, loss: 0.0004587762523442507\n",
      "step: 1564, loss: 0.0006307511939667165\n",
      "step: 1565, loss: 0.0009794668294489384\n",
      "step: 1566, loss: 0.005815868265926838\n",
      "step: 1567, loss: 0.007228041999042034\n",
      "step: 1568, loss: 0.08519665896892548\n",
      "step: 1569, loss: 0.000877326587215066\n",
      "step: 1570, loss: 0.001154033001512289\n",
      "step: 1571, loss: 0.0005763491499237716\n",
      "step: 1572, loss: 0.00174797757063061\n",
      "step: 1573, loss: 0.005018759518861771\n",
      "step: 1574, loss: 0.0005825122934766114\n",
      "step: 1575, loss: 0.0025385874323546886\n",
      "step: 1576, loss: 0.0012821226846426725\n",
      "step: 1577, loss: 0.000426321174018085\n",
      "step: 1578, loss: 0.001368097960948944\n",
      "step: 1579, loss: 0.026559824123978615\n",
      "step: 1580, loss: 0.0005129678174853325\n",
      "step: 1581, loss: 0.00026536735822446644\n",
      "step: 1582, loss: 0.000484427553601563\n",
      "step: 1583, loss: 0.0003331160987727344\n",
      "step: 1584, loss: 0.0008236278663389385\n",
      "step: 1585, loss: 0.0003699319495353848\n",
      "step: 1586, loss: 0.0018964293412864208\n",
      "step: 1587, loss: 0.0008784806705079973\n",
      "step: 1588, loss: 0.09406685084104538\n",
      "step: 1589, loss: 0.0005618444993160665\n",
      "step: 1590, loss: 0.0007787905051372945\n",
      "step: 1591, loss: 0.0006475969566963613\n",
      "step: 1592, loss: 0.0009067311184480786\n",
      "step: 1593, loss: 0.0008465293212793767\n",
      "step: 1594, loss: 0.0009823994478210807\n",
      "step: 1595, loss: 0.000986712402664125\n",
      "step: 1596, loss: 0.0006621986394748092\n",
      "step: 1597, loss: 0.0004704811726696789\n",
      "step: 1598, loss: 0.0002424583217361942\n",
      "step: 1599, loss: 0.002527476754039526\n",
      "step: 1600, loss: 0.0008524192962795496\n",
      "step: 1601, loss: 0.0015194462612271309\n",
      "step: 1602, loss: 0.0005777537589892745\n",
      "step: 1603, loss: 0.000549859250895679\n",
      "step: 1604, loss: 0.0009176013409160078\n",
      "step: 1605, loss: 0.01862829364836216\n",
      "step: 1606, loss: 0.0005100627895444632\n",
      "step: 1607, loss: 0.012810631655156612\n",
      "step: 1608, loss: 0.0004915586323477328\n",
      "step: 1609, loss: 0.00039555065450258553\n",
      "step: 1610, loss: 0.0009928878862410784\n",
      "step: 1611, loss: 0.0004618813982233405\n",
      "step: 1612, loss: 0.00031478493474423885\n",
      "step: 1613, loss: 0.000440522184362635\n",
      "step: 1614, loss: 0.0003646924742497504\n",
      "step: 1615, loss: 0.001084249815903604\n",
      "step: 1616, loss: 0.0008230273961089551\n",
      "step: 1617, loss: 0.001133646466769278\n",
      "step: 1618, loss: 0.0006538827437907457\n",
      "step: 1619, loss: 0.0005607388447970152\n",
      "step: 1620, loss: 0.0007138517685234547\n",
      "step: 1621, loss: 0.0009729657904244959\n",
      "step: 1622, loss: 0.0006834740051999688\n",
      "step: 1623, loss: 0.004523287992924452\n",
      "step: 1624, loss: 0.0020539038814604282\n",
      "step: 1625, loss: 0.0004197901871521026\n",
      "step: 1626, loss: 0.0007235383382067084\n",
      "step: 1627, loss: 0.0011397494236007333\n",
      "step: 1628, loss: 0.0011479189852252603\n",
      "step: 1629, loss: 0.02785714715719223\n",
      "step: 1630, loss: 0.000896769983228296\n",
      "step: 1631, loss: 0.0005457005463540554\n",
      "step: 1632, loss: 0.000566107511986047\n",
      "step: 1633, loss: 0.0006995522999204695\n",
      "step: 1634, loss: 0.0008203735924325883\n",
      "step: 1635, loss: 0.00043321825796738267\n",
      "step: 1636, loss: 0.0005195252015255392\n",
      "step: 1637, loss: 0.0007339103030972183\n",
      "step: 1638, loss: 0.003432686673477292\n",
      "step: 1639, loss: 0.0005981129943393171\n",
      "step: 1640, loss: 0.0008303748327307403\n",
      "step: 1641, loss: 0.00044047593837603927\n",
      "step: 1642, loss: 0.0016072167782112956\n",
      "step: 1643, loss: 0.03240256384015083\n",
      "step: 1644, loss: 0.0006793078500777483\n",
      "step: 1645, loss: 0.004725903272628784\n",
      "step: 1646, loss: 0.0006437291740439832\n",
      "step: 1647, loss: 0.0008144737803377211\n",
      "step: 1648, loss: 0.00046320981346070766\n",
      "step: 1649, loss: 0.0007814821437932551\n",
      "step: 1650, loss: 0.00048776555922813714\n",
      "step: 1651, loss: 0.0008665609639137983\n",
      "step: 1652, loss: 0.01666256971657276\n",
      "step: 1653, loss: 0.0018122729379683733\n",
      "step: 1654, loss: 0.004121331498026848\n",
      "step: 1655, loss: 0.0005273568094708025\n",
      "step: 1656, loss: 0.0008308480028063059\n",
      "step: 1657, loss: 0.021308865398168564\n",
      "step: 1658, loss: 0.0007415752625092864\n",
      "step: 1659, loss: 0.0005936298402957618\n",
      "step: 1660, loss: 0.00044511453597806394\n",
      "step: 1661, loss: 0.0012686607660725713\n",
      "step: 1662, loss: 0.0004180754767730832\n",
      "step: 1663, loss: 0.0005288114189170301\n",
      "step: 1664, loss: 0.0003977183951064944\n",
      "step: 1665, loss: 0.0002934640215244144\n",
      "step: 1666, loss: 0.000739240669645369\n",
      "step: 1667, loss: 0.001140420907177031\n",
      "step: 1668, loss: 0.0008842192473821342\n",
      "step: 1669, loss: 0.0013487234245985746\n",
      "step: 1670, loss: 0.00048158306162804365\n",
      "step: 1671, loss: 0.0005807862617075443\n",
      "step: 1672, loss: 0.002227442804723978\n",
      "step: 1673, loss: 0.00045217329170554876\n",
      "step: 1674, loss: 0.030274230986833572\n",
      "step: 1675, loss: 0.00058199861086905\n",
      "step: 1676, loss: 0.0006206948892213404\n",
      "step: 1677, loss: 0.0007382530020549893\n",
      "step: 1678, loss: 0.0015825845766812563\n",
      "step: 1679, loss: 0.0009400263661518693\n",
      "step: 1680, loss: 0.0004856385930906981\n",
      "step: 1681, loss: 0.009904916398227215\n",
      "step: 1682, loss: 0.001043759286403656\n",
      "step: 1683, loss: 0.000833915313705802\n",
      "step: 1684, loss: 0.0006097849691286683\n",
      "step: 1685, loss: 0.011196546256542206\n",
      "step: 1686, loss: 0.0004559592925943434\n",
      "step: 1687, loss: 0.0005633124965243042\n",
      "step: 1688, loss: 0.0006056656129658222\n",
      "step: 1689, loss: 0.0010814950801432133\n",
      "step: 1690, loss: 0.0012150597758591175\n",
      "step: 1691, loss: 0.0006394365918822587\n",
      "step: 1692, loss: 0.000639485486317426\n",
      "step: 1693, loss: 0.04039977863430977\n",
      "step: 1694, loss: 0.0006561517948284745\n",
      "step: 1695, loss: 0.012009058147668839\n",
      "step: 1696, loss: 0.0012921731686219573\n",
      "step: 1697, loss: 0.0006565318908542395\n",
      "step: 1698, loss: 0.00038292870158329606\n",
      "step: 1699, loss: 0.04397573322057724\n",
      "step: 1700, loss: 0.0005072271451354027\n",
      "step: 1701, loss: 0.0006637207698076963\n",
      "step: 1706, loss: 0.0004986404092051089\n",
      "step: 1707, loss: 0.00028040463803336024\n",
      "step: 1708, loss: 0.000369138055248186\n",
      "step: 1709, loss: 0.000464898650534451\n",
      "step: 1710, loss: 0.000464802113128826\n",
      "step: 1711, loss: 0.0006729289889335632\n",
      "step: 1712, loss: 0.0008000769303180277\n",
      "step: 1713, loss: 0.0006521042669191957\n",
      "step: 1714, loss: 0.0011797707993537188\n",
      "step: 1715, loss: 0.00026915091439150274\n",
      "step: 1716, loss: 0.0005837127682752907\n",
      "step: 1717, loss: 0.0006690690061077476\n",
      "step: 1718, loss: 0.0092883026227355\n",
      "step: 1719, loss: 0.01053229533135891\n",
      "step: 1720, loss: 0.0005195645499043167\n",
      "step: 1721, loss: 0.0008682733168825507\n",
      "step: 1722, loss: 0.0007095918990671635\n",
      "step: 1723, loss: 0.0012890170328319073\n",
      "step: 1724, loss: 0.0007207220769487321\n",
      "step: 1725, loss: 0.001454043434932828\n",
      "step: 1726, loss: 0.000626757275313139\n",
      "step: 1727, loss: 0.0003231413720641285\n",
      "step: 1728, loss: 0.038181088864803314\n",
      "step: 1729, loss: 0.0007504990207962692\n",
      "step: 1730, loss: 0.0005173669196665287\n",
      "step: 1731, loss: 0.00033458531834185123\n",
      "step: 1732, loss: 0.043587543070316315\n",
      "step: 1733, loss: 0.000410543754696846\n",
      "step: 1734, loss: 0.0009714832995086908\n",
      "step: 1735, loss: 0.0003438812564127147\n",
      "step: 1736, loss: 0.00036143153556622565\n",
      "step: 1737, loss: 0.006019840948283672\n",
      "step: 1738, loss: 0.0013123499229550362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1739, loss: 0.001587103703059256\n",
      "step: 1740, loss: 0.001233970746397972\n",
      "step: 1741, loss: 0.03348761051893234\n",
      "step: 1742, loss: 0.00046690955059602857\n",
      "step: 1743, loss: 0.0006325588328763843\n",
      "step: 1744, loss: 0.18094702064990997\n",
      "step: 1745, loss: 0.00028830597875639796\n",
      "step: 1746, loss: 0.00025484751677140594\n",
      "step: 1747, loss: 0.00356145272962749\n",
      "step: 1748, loss: 0.001969809178262949\n",
      "step: 1749, loss: 0.0005023680860176682\n",
      "step: 1750, loss: 0.0007718189735896885\n",
      "step: 1751, loss: 0.0003662451053969562\n",
      "step: 1752, loss: 0.009356919676065445\n",
      "step: 1753, loss: 0.0005278369062580168\n",
      "step: 1754, loss: 0.00022349029313772917\n",
      "step: 1755, loss: 0.0017728913808241487\n",
      "step: 1756, loss: 0.0005654566339217126\n",
      "step: 1757, loss: 0.006315131206065416\n",
      "step: 1758, loss: 0.0008524723816663027\n",
      "step: 1759, loss: 0.025322770699858665\n",
      "step: 1760, loss: 0.0050970204174518585\n",
      "step: 1761, loss: 0.0004545058764051646\n",
      "step: 1762, loss: 0.0012792828492820263\n",
      "step: 1763, loss: 0.0010145591804757714\n",
      "step: 1764, loss: 0.0010990584269165993\n",
      "step: 1765, loss: 0.04526066407561302\n",
      "step: 1766, loss: 0.0004832354898098856\n",
      "step: 1767, loss: 0.00034632525057531893\n",
      "step: 1768, loss: 0.0005180722801014781\n",
      "step: 1769, loss: 0.0006365936133079231\n",
      "step: 1770, loss: 0.00044486383558250964\n",
      "step: 1771, loss: 0.001139539061114192\n",
      "step: 1772, loss: 0.024651572108268738\n",
      "step: 1773, loss: 0.020815925672650337\n",
      "step: 1774, loss: 0.0003990305704064667\n",
      "step: 1775, loss: 0.0009094682172872126\n",
      "step: 1776, loss: 0.0005926999729126692\n",
      "step: 1777, loss: 0.00041419971967116\n",
      "step: 1778, loss: 0.0006186314858496189\n",
      "step: 1779, loss: 0.00040644989348948\n",
      "step: 1780, loss: 0.0014489759923890233\n",
      "step: 1781, loss: 0.0003305503341834992\n",
      "step: 1782, loss: 0.0005886922590434551\n",
      "step: 1783, loss: 0.002146754879504442\n",
      "step: 1784, loss: 0.00024708930868655443\n",
      "step: 1785, loss: 0.0004136866773478687\n",
      "step: 1786, loss: 0.0018685865215957165\n",
      "step: 1787, loss: 0.0013618625234812498\n",
      "step: 1788, loss: 0.0013137769419699907\n",
      "step: 1789, loss: 0.0013501510256901383\n",
      "step: 1790, loss: 0.0006681392551399767\n",
      "step: 1791, loss: 0.00036532056401483715\n",
      "step: 1792, loss: 0.0008823651005513966\n",
      "step: 1793, loss: 0.0036882320418953896\n",
      "step: 1794, loss: 0.00032524683047086\n",
      "step: 1795, loss: 0.028062522411346436\n",
      "step: 1796, loss: 0.0006992343114688993\n",
      "step: 1797, loss: 0.0004453504516277462\n",
      "step: 1798, loss: 0.00029406469548121095\n",
      "step: 1799, loss: 0.07155635207891464\n",
      "step: 1800, loss: 0.0006506352219730616\n",
      "step: 1801, loss: 0.0010051995050162077\n",
      "step: 1802, loss: 0.00038444699021056294\n",
      "step: 1803, loss: 0.0005913145141676068\n",
      "step: 1804, loss: 0.0005349356215447187\n",
      "step: 1805, loss: 0.0018227825639769435\n",
      "step: 1806, loss: 0.0008476270595565438\n",
      "step: 1807, loss: 0.0008025187416933477\n",
      "step: 1808, loss: 0.000843944784719497\n",
      "step: 1809, loss: 0.0003776914381887764\n",
      "step: 1810, loss: 0.0005768871051259339\n",
      "step: 1811, loss: 0.0005568471970036626\n",
      "step: 1812, loss: 0.0010657120728865266\n",
      "step: 1813, loss: 0.0006192289874888957\n",
      "step: 1814, loss: 0.0006588547839783132\n",
      "step: 1815, loss: 0.0003410173812881112\n",
      "step: 1816, loss: 0.0005422933027148247\n",
      "step: 1817, loss: 0.0006819564732722938\n",
      "step: 1818, loss: 0.0035617316607385874\n",
      "step: 1819, loss: 0.0006436111289076507\n",
      "step: 1820, loss: 0.0007022513309493661\n",
      "step: 1821, loss: 0.000475997687317431\n",
      "step: 1822, loss: 0.0006462398450821638\n",
      "step: 1823, loss: 0.0007429391262121499\n",
      "step: 1824, loss: 0.0007456848979927599\n",
      "step: 1825, loss: 0.0010060317581519485\n",
      "step: 1826, loss: 0.0008382817613892257\n",
      "step: 1827, loss: 0.012505647726356983\n",
      "step: 1828, loss: 0.00037601045914925635\n",
      "step: 1829, loss: 0.0010171111207455397\n",
      "step: 1830, loss: 0.0005528632318601012\n",
      "step: 1831, loss: 0.0003027670900337398\n",
      "step: 1832, loss: 0.00040176394395530224\n",
      "step: 1833, loss: 0.004952837247401476\n",
      "step: 1834, loss: 0.0006263626855798066\n",
      "step: 1835, loss: 0.000605617300607264\n",
      "step: 1836, loss: 0.0006634615128859878\n",
      "step: 1837, loss: 0.0006323567940853536\n",
      "step: 1838, loss: 0.013214427046477795\n",
      "step: 1839, loss: 0.026301031932234764\n",
      "step: 1840, loss: 0.00023272352700587362\n",
      "step: 1841, loss: 0.0009385955054312944\n",
      "step: 1842, loss: 0.0005596330156549811\n",
      "step: 1843, loss: 0.0007035289891064167\n",
      "step: 1844, loss: 0.0004970260197296739\n",
      "step: 1845, loss: 0.00029842666117474437\n",
      "step: 1846, loss: 0.0007925188401713967\n",
      "step: 1847, loss: 0.0008754658047109842\n",
      "step: 1848, loss: 0.0009386608726345003\n",
      "step: 1849, loss: 0.0002973449300043285\n",
      "step: 1850, loss: 0.0017460859380662441\n",
      "step: 1851, loss: 0.0003602601937018335\n",
      "step: 1852, loss: 0.030768774449825287\n",
      "step: 1853, loss: 0.0008735277224332094\n",
      "step: 1854, loss: 0.0008716270094737411\n",
      "step: 1855, loss: 0.0003386818862054497\n",
      "step: 1856, loss: 0.0004190858453512192\n",
      "step: 1857, loss: 0.0004430392000358552\n",
      "step: 1858, loss: 0.000504816765896976\n",
      "step: 1859, loss: 0.0009729638113640249\n",
      "step: 1860, loss: 0.0017077752854675055\n",
      "step: 1861, loss: 0.0002756782341748476\n",
      "step: 1862, loss: 0.0004538827342912555\n",
      "step: 1863, loss: 0.0007562144892290235\n",
      "step: 1864, loss: 0.000602198182605207\n",
      "step: 1865, loss: 0.00029279175214469433\n",
      "step: 1866, loss: 0.0005634921253658831\n",
      "step: 1867, loss: 0.0004355265700723976\n",
      "step: 1868, loss: 0.017689932137727737\n",
      "step: 1869, loss: 0.00017454526096116751\n",
      "step: 1870, loss: 0.0004818596935365349\n",
      "step: 1871, loss: 0.0394851453602314\n",
      "step: 1872, loss: 0.0011789093259721994\n",
      "step: 1873, loss: 0.0004845708899665624\n",
      "step: 1874, loss: 0.0003117375308647752\n",
      "step: 1875, loss: 0.00027752897585742176\n",
      "step: 1876, loss: 0.0004076746408827603\n",
      "step: 1877, loss: 0.0006247440469451249\n",
      "step: 1878, loss: 0.000535211234819144\n",
      "step: 1879, loss: 0.00031143228989094496\n",
      "step: 1880, loss: 0.025185709819197655\n",
      "step: 1881, loss: 0.032889559864997864\n",
      "step: 1882, loss: 0.00040669224108569324\n",
      "step: 1883, loss: 0.0002935664670076221\n",
      "step: 1884, loss: 0.0005687210941687226\n",
      "step: 1885, loss: 0.0002687525120563805\n",
      "step: 1886, loss: 0.0005411479505710304\n",
      "step: 1887, loss: 0.0006445582839660347\n",
      "step: 1888, loss: 0.0008549783960916102\n",
      "step: 1889, loss: 0.0002683094935491681\n",
      "step: 1890, loss: 0.0005922322161495686\n",
      "step: 1891, loss: 0.045878175646066666\n",
      "step: 1892, loss: 0.0012865928001701832\n",
      "step: 1893, loss: 0.0005683790659531951\n",
      "step: 1894, loss: 0.021684257313609123\n",
      "step: 1895, loss: 0.0005825128173455596\n",
      "step: 1896, loss: 0.00039453041972592473\n",
      "step: 1897, loss: 0.00043198608909733593\n",
      "step: 1898, loss: 0.00035460450453683734\n",
      "step: 1899, loss: 0.0006963673513382673\n",
      "step: 1900, loss: 0.0005299319745972753\n",
      "step: 1901, loss: 0.000441642856458202\n",
      "step: 1902, loss: 0.0003282907709944993\n",
      "step: 1903, loss: 0.0004481580981519073\n",
      "step: 1904, loss: 0.00030701758805662394\n",
      "step: 1905, loss: 0.0002952253562398255\n",
      "step: 1906, loss: 0.0006283316761255264\n",
      "step: 1907, loss: 0.0006416381220333278\n",
      "step: 1908, loss: 0.0006460324511863291\n",
      "step: 1909, loss: 0.00023787954705767334\n",
      "step: 1910, loss: 0.0006912550888955593\n",
      "step: 1911, loss: 0.0011038531083613634\n",
      "step: 1912, loss: 0.0015006884932518005\n",
      "step: 1913, loss: 0.0005853582988493145\n",
      "step: 1914, loss: 0.00035772655974142253\n",
      "step: 1915, loss: 0.00018258584896102548\n",
      "step: 1916, loss: 0.0006161944475024939\n",
      "step: 1917, loss: 0.0002703809004742652\n",
      "step: 1918, loss: 0.0003119874163530767\n",
      "step: 1919, loss: 0.00036690427805297077\n",
      "step: 1920, loss: 0.0012366895098239183\n",
      "step: 1921, loss: 0.0005635042325593531\n",
      "step: 1922, loss: 0.00049626111285761\n",
      "step: 1923, loss: 0.0005365140968933702\n",
      "step: 1924, loss: 0.006869128905236721\n",
      "step: 1925, loss: 0.0007501621730625629\n",
      "step: 1926, loss: 0.00026389965205453336\n",
      "step: 1927, loss: 0.00047449025441892445\n",
      "step: 1928, loss: 0.00035291429958306253\n",
      "step: 1929, loss: 0.00043873724644072354\n",
      "step: 1930, loss: 0.0002909448812715709\n",
      "step: 1931, loss: 0.0002649569360073656\n",
      "step: 1932, loss: 0.00022986829571891576\n",
      "step: 1933, loss: 0.00029960204847157\n",
      "step: 1934, loss: 0.04468915984034538\n",
      "step: 1935, loss: 0.0003806144814006984\n",
      "step: 1936, loss: 0.00041130147292278707\n",
      "step: 1937, loss: 0.0004594353085849434\n",
      "step: 1938, loss: 0.0008710075635462999\n",
      "step: 1939, loss: 0.0004638030659407377\n",
      "step: 1940, loss: 0.00032476347405463457\n",
      "step: 1941, loss: 0.0005651557585224509\n",
      "step: 1942, loss: 0.0007928984123282135\n",
      "step: 1943, loss: 0.0008150748326443136\n",
      "step: 1944, loss: 0.0006266393465921283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1945, loss: 0.0006486885249614716\n",
      "step: 1946, loss: 0.0006495903944596648\n",
      "step: 1947, loss: 0.0008171597146429121\n",
      "step: 1948, loss: 0.000508206314407289\n",
      "step: 1949, loss: 0.0011400885414332151\n",
      "step: 1950, loss: 0.0007058767951093614\n",
      "step: 1951, loss: 0.00046683382242918015\n",
      "step: 1952, loss: 0.002036853926256299\n",
      "step: 1953, loss: 0.0003881593293044716\n",
      "step: 1954, loss: 0.0007766684284433722\n",
      "step: 1955, loss: 0.0004665809392463416\n",
      "step: 1956, loss: 0.0010541188530623913\n",
      "step: 1957, loss: 0.023690881207585335\n",
      "step: 1958, loss: 0.0005678129964508116\n",
      "step: 1959, loss: 0.0003178217157255858\n",
      "step: 1960, loss: 0.0036233102437108755\n",
      "step: 1961, loss: 0.0009896332630887628\n",
      "step: 1962, loss: 0.0010039546759799123\n",
      "step: 1963, loss: 0.0021780477836728096\n",
      "step: 1964, loss: 0.0006071307579986751\n",
      "step: 1965, loss: 0.0004449106636457145\n",
      "step: 1966, loss: 0.00037852569948881865\n",
      "step: 1967, loss: 0.0005995045648887753\n",
      "step: 1968, loss: 0.0013981149531900883\n",
      "step: 1969, loss: 0.00036707575782202184\n",
      "step: 1970, loss: 0.001518080709502101\n",
      "step: 1971, loss: 0.023071784526109695\n",
      "step: 1972, loss: 0.0007187819574028254\n",
      "step: 1973, loss: 0.000399680167902261\n",
      "step: 1974, loss: 0.000435356458183378\n",
      "step: 1975, loss: 0.0003998283063992858\n",
      "step: 1976, loss: 0.035534411668777466\n",
      "step: 1977, loss: 0.011508149094879627\n",
      "step: 1978, loss: 0.004981926176697016\n",
      "step: 1979, loss: 0.000302482774714008\n",
      "step: 1980, loss: 0.0005950690247118473\n",
      "step: 1981, loss: 0.0008243405027315021\n",
      "step: 1982, loss: 0.0004937824560329318\n",
      "step: 1983, loss: 0.0014239219017326832\n",
      "step: 1984, loss: 0.0005271208356134593\n",
      "step: 1985, loss: 0.0004140780074521899\n",
      "step: 1986, loss: 0.000217997410800308\n",
      "step: 1987, loss: 0.0039032853674143553\n",
      "step: 1988, loss: 0.0004502751980908215\n",
      "step: 1989, loss: 0.0004981374368071556\n",
      "step: 1990, loss: 0.02565978281199932\n",
      "step: 1991, loss: 0.0004392564296722412\n",
      "step: 1992, loss: 0.0006896976847201586\n",
      "step: 1993, loss: 0.0003239221405237913\n",
      "step: 1994, loss: 0.0005247298395261168\n",
      "step: 1995, loss: 0.0005070673068985343\n",
      "step: 1996, loss: 0.00030723772943019867\n",
      "step: 1997, loss: 0.0007411049446091056\n",
      "step: 1998, loss: 0.00017085205763578415\n",
      "step: 1999, loss: 0.00029462651582434773\n",
      "step: 2000, loss: 0.0004386059590615332\n",
      "step: 2001, loss: 0.00023780958144925535\n",
      "step: 2002, loss: 0.0003302631084807217\n",
      "step: 2003, loss: 0.032818134874105453\n",
      "step: 2004, loss: 0.0004103255341760814\n",
      "step: 2005, loss: 0.00037127011455595493\n",
      "step: 2006, loss: 0.00046440507867373526\n",
      "step: 2007, loss: 0.0013883825158700347\n",
      "step: 2008, loss: 0.0006886866176500916\n",
      "step: 2009, loss: 0.02734842523932457\n",
      "step: 2010, loss: 0.00045452304766513407\n",
      "step: 2011, loss: 0.044550977647304535\n",
      "step: 2012, loss: 0.0010019431356340647\n",
      "step: 2013, loss: 0.0030322447419166565\n",
      "step: 2014, loss: 0.0005610338412225246\n",
      "step: 2015, loss: 0.0006116349250078201\n",
      "step: 2016, loss: 0.00026284149498678744\n",
      "step: 2017, loss: 0.0005151886725798249\n",
      "step: 2018, loss: 0.0008070308831520379\n",
      "step: 2019, loss: 0.03532824292778969\n",
      "step: 2020, loss: 0.001084141549654305\n",
      "step: 2021, loss: 0.00022427499061450362\n",
      "step: 2022, loss: 0.0003718101652339101\n",
      "step: 2023, loss: 0.0008921927073970437\n",
      "step: 2024, loss: 0.0006194873712956905\n",
      "step: 2025, loss: 0.00031969978590495884\n",
      "step: 2026, loss: 0.0005229012458585203\n",
      "step: 2027, loss: 0.0005363714299164712\n",
      "step: 2028, loss: 0.000328186055412516\n",
      "step: 2029, loss: 0.01689307764172554\n",
      "step: 2030, loss: 0.00059700250858441\n",
      "step: 2031, loss: 0.0006013839156366885\n",
      "step: 2032, loss: 0.0006730929599143565\n",
      "step: 2033, loss: 0.0002469863975420594\n",
      "step: 2034, loss: 0.03622244670987129\n",
      "step: 2035, loss: 0.0006508185178972781\n",
      "step: 2036, loss: 0.0005644678603857756\n",
      "step: 2037, loss: 0.0006250199512578547\n",
      "step: 2038, loss: 0.0006246092380024493\n",
      "step: 2039, loss: 0.00023778321337886155\n",
      "step: 2040, loss: 0.00045642434270121157\n",
      "step: 2041, loss: 0.0006914850091561675\n",
      "step: 2042, loss: 0.006162088364362717\n",
      "step: 2043, loss: 0.0003663655079435557\n",
      "step: 2044, loss: 0.000631765928119421\n",
      "step: 2045, loss: 0.0034168849233537912\n",
      "step: 2046, loss: 0.0003862570156343281\n",
      "step: 2047, loss: 0.0003761022526305169\n",
      "step: 2048, loss: 0.024041278287768364\n",
      "step: 2049, loss: 0.00028918019961565733\n",
      "step: 2050, loss: 0.00022850507230032235\n",
      "step: 2051, loss: 0.01361863873898983\n",
      "step: 2052, loss: 0.00024857104290276766\n",
      "step: 2053, loss: 0.0005926591693423688\n",
      "step: 2054, loss: 0.00028496369486674666\n",
      "step: 2055, loss: 0.00037804763996973634\n",
      "step: 2056, loss: 0.0004598511732183397\n",
      "step: 2057, loss: 0.00033005874138325453\n",
      "step: 2058, loss: 0.0005419809021987021\n",
      "step: 2059, loss: 0.0004460517084226012\n",
      "step: 2060, loss: 0.0002931781637016684\n",
      "step: 2061, loss: 0.00015861786960158497\n",
      "step: 2062, loss: 0.00027158783632330596\n",
      "step: 2063, loss: 0.0005305156228132546\n",
      "step: 2064, loss: 0.00028420903254300356\n",
      "step: 2065, loss: 0.00020651021623052657\n",
      "step: 2066, loss: 0.0001875267189461738\n",
      "step: 2067, loss: 0.00016481989587191492\n",
      "step: 2068, loss: 0.00043387673213146627\n",
      "step: 2069, loss: 0.005792619660496712\n",
      "step: 2070, loss: 0.0003653864550869912\n",
      "step: 2071, loss: 0.0002616049605421722\n",
      "step: 2072, loss: 0.00025827655917964876\n",
      "step: 2073, loss: 0.0006364071159623563\n",
      "step: 2074, loss: 0.00034404813777655363\n",
      "step: 2075, loss: 0.0003912159299943596\n",
      "step: 2076, loss: 0.0002811092999763787\n",
      "step: 2077, loss: 0.0004792500985786319\n",
      "step: 2078, loss: 0.000337039353325963\n",
      "step: 2079, loss: 0.00036192830884829164\n",
      "step: 2080, loss: 0.00038258268614299595\n",
      "step: 2081, loss: 0.0003885421610902995\n",
      "step: 2082, loss: 0.00024399894755333662\n",
      "step: 2083, loss: 0.02745400369167328\n",
      "step: 2084, loss: 0.0006769219180569053\n",
      "step: 2085, loss: 0.000752689375076443\n",
      "step: 2086, loss: 0.0002291216078447178\n",
      "step: 2087, loss: 0.0008861772948876023\n",
      "step: 2088, loss: 0.0003138069296255708\n",
      "step: 2089, loss: 0.0003787339373957366\n",
      "step: 2090, loss: 0.0005674948915839195\n",
      "step: 2091, loss: 0.00040449592052027583\n",
      "step: 2092, loss: 0.0002560455468483269\n",
      "step: 2093, loss: 0.0007795195560902357\n",
      "step: 2094, loss: 0.0005783014348708093\n",
      "step: 2095, loss: 0.0002946890308521688\n",
      "step: 2096, loss: 0.00023946551664266735\n",
      "step: 2097, loss: 0.000538646534550935\n",
      "step: 2098, loss: 0.00024039387062657624\n",
      "step: 2099, loss: 0.0003647451812867075\n",
      "step: 2100, loss: 0.0005239531164988875\n",
      "step: 2101, loss: 0.0002438415976939723\n",
      "step: 2102, loss: 0.001197397243231535\n",
      "step: 2103, loss: 0.00024788794689811766\n",
      "step: 2104, loss: 0.0006215496105141938\n",
      "step: 2105, loss: 0.0005459001986309886\n",
      "step: 2106, loss: 0.012189910747110844\n",
      "step: 2107, loss: 0.03515896201133728\n",
      "step: 2108, loss: 0.0009846973698586226\n",
      "step: 2109, loss: 0.0005746794631704688\n",
      "step: 2110, loss: 0.00036025463487021625\n",
      "step: 2111, loss: 0.00040025744237937033\n",
      "step: 2112, loss: 0.00021661928622052073\n",
      "step: 2113, loss: 0.00023888151918072253\n",
      "step: 2114, loss: 0.0003202682128176093\n",
      "step: 2115, loss: 0.0013688765466213226\n",
      "step: 2116, loss: 0.0004865688970312476\n",
      "step: 2117, loss: 0.00999939814209938\n",
      "step: 2118, loss: 0.00025397897115908563\n",
      "step: 2119, loss: 0.000797069224063307\n",
      "step: 2120, loss: 0.00018185986846219748\n",
      "step: 2121, loss: 0.0003630873980000615\n",
      "step: 2122, loss: 0.0003112673875875771\n",
      "step: 2123, loss: 0.0006687564309686422\n",
      "step: 2124, loss: 0.00042680572369135916\n",
      "step: 2125, loss: 0.0002816172200255096\n",
      "step: 2126, loss: 0.0002220457827206701\n",
      "step: 2127, loss: 0.0014976303791627288\n",
      "step: 2128, loss: 0.0002517235407140106\n",
      "step: 2129, loss: 0.0001879305054899305\n",
      "step: 2130, loss: 0.0005863085971213877\n",
      "step: 2131, loss: 0.00046257671783678234\n",
      "step: 2132, loss: 0.00026882439851760864\n",
      "step: 2133, loss: 0.00025493832072243094\n",
      "step: 2134, loss: 0.0002095923264278099\n",
      "step: 2135, loss: 0.000277643179288134\n",
      "step: 2136, loss: 0.07723773270845413\n",
      "step: 2137, loss: 0.00033163957414217293\n",
      "step: 2138, loss: 0.00039628398371860385\n",
      "step: 2139, loss: 0.0005455745849758387\n",
      "step: 2140, loss: 0.00025080272462219\n",
      "step: 2141, loss: 0.012014120817184448\n",
      "step: 2142, loss: 0.0002950382186099887\n",
      "step: 2143, loss: 0.000576755846850574\n",
      "step: 2144, loss: 0.0005151013610884547\n",
      "step: 2145, loss: 0.0006819942500442266\n",
      "step: 2146, loss: 0.0005203933687880635\n",
      "step: 2147, loss: 0.00019247039745096117\n",
      "step: 2148, loss: 0.0003008717903867364\n",
      "step: 2149, loss: 0.0007637945818714797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2150, loss: 0.000636063574347645\n",
      "step: 2151, loss: 0.00044212699867784977\n",
      "step: 2152, loss: 0.0006939757149666548\n",
      "step: 2153, loss: 0.0006546130171045661\n",
      "step: 2154, loss: 0.0006085311179049313\n",
      "step: 2155, loss: 0.0008610913064330816\n",
      "step: 2156, loss: 0.0007649746257811785\n",
      "step: 2157, loss: 0.0002820778754539788\n",
      "step: 2158, loss: 0.0006422508740797639\n",
      "step: 2159, loss: 0.0006089616217650473\n",
      "step: 2160, loss: 0.00024175969883799553\n",
      "step: 2161, loss: 0.0010292103979736567\n",
      "step: 2162, loss: 0.000518468557856977\n",
      "step: 2163, loss: 0.00022540600912179798\n",
      "step: 2164, loss: 0.0535660944879055\n",
      "step: 2165, loss: 0.0007287567132152617\n",
      "step: 2166, loss: 0.0003969747922383249\n",
      "step: 2167, loss: 0.00012366224837023765\n",
      "step: 2168, loss: 0.0006556889275088906\n",
      "step: 2169, loss: 0.0003036729176528752\n",
      "step: 2170, loss: 0.0003247674903832376\n",
      "step: 2171, loss: 0.0002542577567510307\n",
      "step: 2172, loss: 0.0021445625461637974\n",
      "step: 2173, loss: 0.0005030063330195844\n",
      "step: 2174, loss: 0.00045091670472174883\n",
      "step: 2175, loss: 0.002417181618511677\n",
      "step: 2176, loss: 0.0007407897501252592\n",
      "step: 2177, loss: 0.00028017975273542106\n",
      "step: 2178, loss: 0.00035664369352161884\n",
      "step: 2179, loss: 0.000504671479575336\n",
      "step: 2180, loss: 0.003941243980079889\n",
      "step: 2181, loss: 0.0004885458038188517\n",
      "step: 2182, loss: 0.0003079684101976454\n",
      "step: 2183, loss: 0.0004028471594210714\n",
      "step: 2184, loss: 0.00041595357470214367\n",
      "step: 2185, loss: 0.06605053693056107\n",
      "step: 2186, loss: 0.0002979279379360378\n",
      "step: 2187, loss: 0.0005657307920046151\n",
      "step: 2188, loss: 0.00048495843657292426\n",
      "step: 2189, loss: 0.0057363929226994514\n",
      "step: 2190, loss: 0.035936370491981506\n",
      "step: 2191, loss: 0.0014428721042349935\n",
      "step: 2192, loss: 0.00033163029002025723\n",
      "step: 2193, loss: 0.00018240427016280591\n",
      "step: 2194, loss: 0.0002997327537741512\n",
      "step: 2195, loss: 0.0003646872646640986\n",
      "step: 2196, loss: 0.001018536277115345\n",
      "step: 2197, loss: 0.0010875563602894545\n",
      "step: 2198, loss: 0.003146668430417776\n",
      "step: 2199, loss: 0.00036209620884619653\n",
      "step: 2200, loss: 0.00043246749555692077\n",
      "step: 2201, loss: 0.000488266785396263\n",
      "step: 2202, loss: 0.00035026457044295967\n",
      "step: 2203, loss: 0.0004920948413200676\n",
      "step: 2204, loss: 0.0002930919290520251\n",
      "step: 2205, loss: 0.000839386077132076\n",
      "step: 2206, loss: 0.0010758629068732262\n",
      "step: 2207, loss: 0.00045278429752215743\n",
      "step: 2208, loss: 0.00041183221037499607\n",
      "step: 2209, loss: 0.00020801530627068132\n",
      "step: 2210, loss: 0.00022145202092360705\n",
      "step: 2211, loss: 0.044860344380140305\n",
      "step: 2212, loss: 0.027047835290431976\n",
      "step: 2213, loss: 0.00048735213931649923\n",
      "step: 2214, loss: 0.0003353738284204155\n",
      "step: 2215, loss: 0.00039407730218954384\n",
      "step: 2216, loss: 0.000416239578044042\n",
      "step: 2217, loss: 0.00050941581139341\n",
      "step: 2218, loss: 0.0003394294180907309\n",
      "step: 2219, loss: 0.0002516958338674158\n",
      "step: 2220, loss: 0.001446276786737144\n",
      "step: 2221, loss: 0.028433971107006073\n",
      "step: 2222, loss: 0.010440332815051079\n",
      "step: 2223, loss: 0.0006382790161296725\n",
      "step: 2224, loss: 0.0002103736624121666\n",
      "step: 2225, loss: 0.00027621330809779465\n",
      "step: 2226, loss: 0.026889590546488762\n",
      "step: 2227, loss: 0.001351055921986699\n",
      "step: 2228, loss: 0.0004922754596918821\n",
      "step: 2229, loss: 0.000375147326849401\n",
      "step: 2230, loss: 0.0003671105660032481\n",
      "step: 2231, loss: 8.709789835847914e-05\n",
      "step: 2232, loss: 0.00028161381487734616\n",
      "step: 2233, loss: 0.0007848035311326385\n",
      "step: 2234, loss: 0.0008494282374158502\n",
      "step: 2235, loss: 0.00031793906236998737\n",
      "step: 2236, loss: 0.00021711568115279078\n",
      "step: 2237, loss: 0.0002683261991478503\n",
      "step: 2238, loss: 0.0004511381557676941\n",
      "step: 2239, loss: 0.0002714757574722171\n",
      "step: 2240, loss: 0.037594620138406754\n",
      "step: 2241, loss: 0.00043908762745559216\n",
      "step: 2242, loss: 0.0018603879725560546\n",
      "step: 2243, loss: 0.0005044487188570201\n",
      "step: 2244, loss: 0.0002713595749810338\n",
      "step: 2245, loss: 0.0002815202751662582\n",
      "step: 2246, loss: 0.00032549965544603765\n",
      "step: 2247, loss: 0.00021140890021342784\n",
      "step: 2248, loss: 0.00021971354726701975\n",
      "step: 2249, loss: 0.0007167041185311973\n",
      "step: 2250, loss: 0.0008227620855905116\n",
      "step: 2251, loss: 0.00027539179427549243\n",
      "step: 2252, loss: 0.00042486086022108793\n",
      "step: 2253, loss: 0.0004353988915681839\n",
      "step: 2254, loss: 0.0007587490836158395\n",
      "step: 2255, loss: 0.0007965920376591384\n",
      "step: 2256, loss: 0.0007274381350725889\n",
      "step: 2257, loss: 0.0003334091161377728\n",
      "step: 2258, loss: 0.0003818793629761785\n",
      "step: 2259, loss: 0.04295776039361954\n",
      "step: 2260, loss: 0.0006816426757723093\n",
      "step: 2261, loss: 0.00025374258984811604\n",
      "step: 2262, loss: 0.000750750012230128\n",
      "step: 2263, loss: 0.00040141388308256865\n",
      "step: 2264, loss: 0.00024116064014378935\n",
      "step: 2265, loss: 0.000662672275211662\n",
      "step: 2266, loss: 0.00828715693205595\n",
      "step: 2267, loss: 0.00040415936382487416\n",
      "step: 2268, loss: 0.00032011742587201297\n",
      "step: 2269, loss: 0.0008225928759202361\n",
      "step: 2270, loss: 0.000148209830513224\n",
      "step: 2271, loss: 0.00026729959063231945\n",
      "step: 2272, loss: 0.0006028441130183637\n",
      "step: 2273, loss: 0.0001716859987936914\n",
      "step: 2274, loss: 0.0003928437945432961\n",
      "step: 2275, loss: 0.0001648830366320908\n",
      "step: 2276, loss: 0.00019389356020838022\n",
      "step: 2277, loss: 0.0004934630705974996\n",
      "step: 2278, loss: 0.00030970334773883224\n",
      "step: 2279, loss: 0.00027934505487792194\n",
      "step: 2280, loss: 0.009629813022911549\n",
      "step: 2281, loss: 0.011146276257932186\n",
      "step: 2282, loss: 0.0003061790484935045\n",
      "step: 2283, loss: 0.00024378506350331008\n",
      "step: 2284, loss: 0.00014850773732177913\n",
      "step: 2285, loss: 0.008251635357737541\n",
      "step: 2286, loss: 0.0003511848917696625\n",
      "step: 2287, loss: 0.03718366473913193\n",
      "step: 2288, loss: 0.00022538928897120059\n",
      "step: 2289, loss: 0.0009452610393054783\n",
      "step: 2290, loss: 0.00023166241589933634\n",
      "step: 2291, loss: 0.00028655503410845995\n",
      "step: 2292, loss: 0.00019207615696359426\n",
      "step: 2293, loss: 0.01543158758431673\n",
      "step: 2294, loss: 0.00035546685103327036\n",
      "step: 2295, loss: 0.0003092534316238016\n",
      "step: 2296, loss: 0.0007463291985914111\n",
      "step: 2297, loss: 0.0006104474305175245\n",
      "step: 2298, loss: 0.0005120798596180975\n",
      "step: 2299, loss: 0.008851384744048119\n",
      "step: 2300, loss: 0.00023897323990240693\n",
      "step: 2301, loss: 0.00036065373569726944\n",
      "step: 2302, loss: 0.02504194900393486\n",
      "step: 2303, loss: 0.0005853928159922361\n",
      "step: 2304, loss: 0.0003120861656498164\n",
      "step: 2305, loss: 0.0006678170175291598\n",
      "step: 2306, loss: 0.0005211063544265926\n",
      "step: 2307, loss: 0.0002692962298169732\n",
      "step: 2308, loss: 0.00041890202555805445\n",
      "step: 2309, loss: 0.0004273777885828167\n",
      "step: 2310, loss: 0.000374628376448527\n",
      "step: 2311, loss: 0.00030739736394025385\n",
      "step: 2312, loss: 0.0007124982657842338\n",
      "step: 2313, loss: 0.0003537041775416583\n",
      "step: 2314, loss: 0.00019725192396435887\n",
      "step: 2315, loss: 0.0003061395254917443\n",
      "step: 2316, loss: 0.000275964179309085\n",
      "step: 2317, loss: 0.00021540971647482365\n",
      "step: 2318, loss: 0.047952428460121155\n",
      "step: 2319, loss: 0.000805996882263571\n",
      "step: 2320, loss: 0.0001499879581388086\n",
      "step: 2321, loss: 0.00038480275543406606\n",
      "step: 2322, loss: 0.00028773726080544293\n",
      "step: 2323, loss: 0.0002561090514063835\n",
      "step: 2324, loss: 0.00024179898900911212\n",
      "step: 2325, loss: 0.038920097053050995\n",
      "step: 2326, loss: 0.00032190189813263714\n",
      "step: 2327, loss: 0.0001433067227480933\n",
      "step: 2328, loss: 0.0001759898295858875\n",
      "step: 2329, loss: 0.0003192398580722511\n",
      "step: 2330, loss: 0.00044727561180479825\n",
      "step: 2331, loss: 0.00013752254017163068\n",
      "step: 2332, loss: 0.00036478976835496724\n",
      "step: 2333, loss: 0.00020881516684312373\n",
      "step: 2334, loss: 0.0004385445499792695\n",
      "step: 2335, loss: 0.0004417191375978291\n",
      "step: 2336, loss: 0.0003823945007752627\n",
      "step: 2337, loss: 0.00020600197603926063\n",
      "step: 2338, loss: 0.0003236713237129152\n",
      "step: 2339, loss: 0.000958980992436409\n",
      "step: 2340, loss: 0.00038578809471800923\n",
      "step: 2341, loss: 0.0001792028924683109\n",
      "step: 2342, loss: 0.00016057792527135462\n",
      "step: 2343, loss: 0.00022900129260960966\n",
      "step: 2344, loss: 0.00013929916894994676\n",
      "step: 2345, loss: 0.00047935423208400607\n",
      "step: 2346, loss: 0.0005175936385057867\n",
      "step: 2347, loss: 0.0005081777926534414\n",
      "step: 2348, loss: 0.00013350950030144304\n",
      "step: 2349, loss: 0.000592354335822165\n",
      "step: 2350, loss: 0.00020126873278059065\n",
      "step: 2351, loss: 0.0003588918480090797\n",
      "step: 2352, loss: 0.0002854402409866452\n",
      "step: 2353, loss: 0.00023246169439516962\n",
      "step: 2354, loss: 0.00028523916262201965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2355, loss: 0.00034657909418456256\n",
      "step: 2356, loss: 0.000255232909694314\n",
      "step: 2357, loss: 0.003727538511157036\n",
      "step: 2358, loss: 0.0002089674526359886\n",
      "step: 2359, loss: 0.0004476085305213928\n",
      "step: 2360, loss: 0.0012747662840411067\n",
      "step: 2361, loss: 0.0003515487478580326\n",
      "step: 2362, loss: 0.00033271111897192895\n",
      "step: 2363, loss: 0.0004489836865104735\n",
      "step: 2364, loss: 0.0008022547699511051\n",
      "step: 2365, loss: 0.0006456378614529967\n",
      "step: 2366, loss: 0.00041735198465175927\n",
      "step: 2367, loss: 0.00016703011351637542\n",
      "step: 2368, loss: 0.00026848362176679075\n",
      "step: 2369, loss: 0.0003002247540280223\n",
      "step: 2370, loss: 0.00012810347834601998\n",
      "step: 2371, loss: 0.0003523631894495338\n",
      "step: 2372, loss: 7.614708010805771e-05\n",
      "step: 2373, loss: 0.00020278450392652303\n",
      "step: 2374, loss: 0.00022514889133162796\n",
      "step: 2375, loss: 0.0002241062611574307\n",
      "step: 2376, loss: 0.00035128119634464383\n",
      "step: 2377, loss: 0.0002817421336658299\n",
      "step: 2378, loss: 0.0002270305121783167\n",
      "step: 2379, loss: 0.0004416437877807766\n",
      "step: 2380, loss: 0.0003003591555170715\n",
      "step: 2381, loss: 0.00029663939494639635\n",
      "step: 2382, loss: 0.0001757624704623595\n",
      "step: 2383, loss: 0.0003218789352104068\n",
      "step: 2384, loss: 0.0008291543344967067\n",
      "step: 2385, loss: 0.0004691818612627685\n",
      "step: 2386, loss: 0.00035479114740155637\n",
      "step: 2387, loss: 0.00013288328773342073\n",
      "step: 2388, loss: 0.0008365765097551048\n",
      "step: 2389, loss: 0.00012534053530544043\n",
      "step: 2390, loss: 0.0001671643549343571\n",
      "step: 2391, loss: 0.00038910878356546164\n",
      "step: 2392, loss: 0.00017268434748984873\n",
      "step: 2393, loss: 0.0002618522848933935\n",
      "step: 2394, loss: 0.00014683636254630983\n",
      "step: 2395, loss: 0.029409848153591156\n",
      "step: 2396, loss: 0.0009285190608352423\n",
      "step: 2397, loss: 0.00026856735348701477\n",
      "step: 2398, loss: 0.000326341949403286\n",
      "step: 2399, loss: 0.000421195785747841\n",
      "step: 2400, loss: 0.00046515729627572\n",
      "step: 2401, loss: 0.0002826724376063794\n",
      "step: 2402, loss: 0.0009037621202878654\n",
      "step: 2403, loss: 0.0006169002735987306\n",
      "step: 2404, loss: 0.00010442579514347017\n",
      "step: 2405, loss: 0.14845654368400574\n",
      "step: 2406, loss: 0.00012669587158598006\n",
      "step: 2407, loss: 0.00036963928141631186\n",
      "step: 2408, loss: 0.00015754165360704064\n",
      "step: 2409, loss: 0.0002784988027997315\n",
      "step: 2410, loss: 0.00032192684011533856\n",
      "step: 2411, loss: 0.0002527132455725223\n",
      "step: 2412, loss: 0.0001766426139511168\n",
      "step: 2413, loss: 0.0002869853633455932\n",
      "step: 2414, loss: 0.0002710761909838766\n",
      "step: 2415, loss: 0.0001677756808931008\n",
      "step: 2416, loss: 0.00019573936879169196\n",
      "step: 2417, loss: 0.00017159378330688924\n",
      "step: 2418, loss: 0.0002017127553699538\n",
      "step: 2419, loss: 0.00017195474356412888\n",
      "step: 2420, loss: 0.0003351664636284113\n",
      "step: 2421, loss: 0.0018578938907012343\n",
      "step: 2422, loss: 0.0003070793754886836\n",
      "step: 2423, loss: 0.0006273889448493719\n",
      "step: 2424, loss: 0.00024301267694681883\n",
      "step: 2425, loss: 0.00046585954260081053\n",
      "step: 2426, loss: 0.0014338094042614102\n",
      "step: 2427, loss: 0.0004974883631803095\n",
      "step: 2428, loss: 0.0002856478386092931\n",
      "step: 2429, loss: 0.0005490239127539098\n",
      "step: 2430, loss: 0.0358608104288578\n",
      "step: 2431, loss: 0.0003645697142928839\n",
      "step: 2432, loss: 0.0001913075684569776\n",
      "step: 2433, loss: 0.008356401696801186\n",
      "step: 2434, loss: 0.0003688170691020787\n",
      "step: 2435, loss: 0.0001775904675014317\n",
      "step: 2436, loss: 0.00013007134839426726\n",
      "step: 2437, loss: 0.00024841708363965154\n",
      "step: 2438, loss: 0.0004359096637926996\n",
      "step: 2439, loss: 0.00018133141566067934\n",
      "step: 2440, loss: 0.053971875458955765\n",
      "step: 2441, loss: 0.0003064374323002994\n",
      "step: 2442, loss: 0.0007635780493728817\n",
      "step: 2443, loss: 0.00019437212904449552\n",
      "step: 2444, loss: 0.0002853837504517287\n",
      "step: 2445, loss: 0.00036095696850679815\n",
      "step: 2446, loss: 0.0002900984254665673\n",
      "step: 2447, loss: 0.00021014190861023962\n",
      "step: 2448, loss: 0.0005107818287797272\n",
      "step: 2449, loss: 0.018215928226709366\n",
      "step: 2450, loss: 0.00017823984671849757\n",
      "step: 2451, loss: 0.01573391631245613\n",
      "step: 2452, loss: 7.012909918557853e-05\n",
      "step: 2453, loss: 0.0007815505377948284\n",
      "step: 2454, loss: 0.0007096698973327875\n",
      "step: 2455, loss: 8.840342343319207e-05\n",
      "step: 2456, loss: 0.00022090328275226057\n",
      "step: 2457, loss: 0.03431032970547676\n",
      "step: 2458, loss: 0.009028559550642967\n",
      "step: 2459, loss: 0.00019195508502889425\n",
      "step: 2460, loss: 0.0002327403490198776\n",
      "step: 2461, loss: 0.0003336257068440318\n",
      "step: 2462, loss: 0.0007170774042606354\n",
      "step: 2463, loss: 0.0005165899638086557\n",
      "step: 2464, loss: 0.0002283471985720098\n",
      "step: 2465, loss: 0.000159721021191217\n",
      "step: 2466, loss: 0.0006466849008575082\n",
      "step: 2467, loss: 0.0003337942762300372\n",
      "step: 2468, loss: 0.0005234384443610907\n",
      "step: 2469, loss: 0.00024166464572772384\n",
      "step: 2470, loss: 0.00018922284652944654\n",
      "step: 2471, loss: 0.00026900062221102417\n",
      "step: 2472, loss: 0.02869659848511219\n",
      "step: 2473, loss: 0.0007837822195142508\n",
      "step: 2474, loss: 0.00027832385967485607\n",
      "step: 2475, loss: 0.00033753205207176507\n",
      "step: 2476, loss: 0.0005958472029305995\n",
      "step: 2477, loss: 0.00016739772399887443\n",
      "step: 2478, loss: 0.00019082067592535168\n",
      "step: 2479, loss: 0.00012299568334128708\n",
      "step: 2480, loss: 0.0008221481693908572\n",
      "step: 2481, loss: 0.00017900617967825383\n",
      "step: 2482, loss: 0.040445197373628616\n",
      "step: 2483, loss: 0.0003784355940297246\n",
      "step: 2484, loss: 0.00015160991461016238\n",
      "step: 2485, loss: 0.003875244176015258\n",
      "step: 2486, loss: 0.00016043250798247755\n",
      "step: 2487, loss: 0.0006909950752742589\n",
      "step: 2488, loss: 0.00023093361232895404\n",
      "step: 2489, loss: 0.0007949969731271267\n",
      "step: 2490, loss: 0.0003960774920415133\n",
      "step: 2491, loss: 0.0001848461979534477\n",
      "step: 2492, loss: 0.00024176144506782293\n",
      "step: 2493, loss: 0.000154074325109832\n",
      "step: 2494, loss: 0.00017536088125780225\n",
      "step: 2495, loss: 0.00028306274907663465\n",
      "step: 2496, loss: 0.0002539343840908259\n",
      "step: 2497, loss: 0.00020627352932933718\n",
      "step: 2498, loss: 0.0017795029561966658\n",
      "step: 2499, loss: 0.00027995440177619457\n",
      "step: 2500, loss: 0.00045837077777832747\n",
      "step: 2501, loss: 0.00030779122607782483\n",
      "step: 2502, loss: 0.0001879031624412164\n",
      "step: 2503, loss: 0.0002182365715270862\n",
      "step: 2504, loss: 0.0001207006280310452\n",
      "step: 2505, loss: 0.00015791830082889646\n",
      "step: 2506, loss: 0.001926085096783936\n",
      "step: 2507, loss: 0.0003270156739745289\n",
      "step: 2508, loss: 0.0003238575882278383\n",
      "step: 2509, loss: 0.00013171238242648542\n",
      "step: 2510, loss: 0.0002867621660698205\n",
      "step: 2511, loss: 0.0005835315096192062\n",
      "step: 2512, loss: 0.00016329229401890188\n",
      "step: 2513, loss: 0.00021219148766249418\n",
      "step: 2514, loss: 0.0004414981522131711\n",
      "step: 2515, loss: 0.042557068169116974\n",
      "step: 2516, loss: 0.0003822442959062755\n",
      "step: 2517, loss: 0.00013929004489909858\n",
      "step: 2518, loss: 0.00014433455362450331\n",
      "step: 2519, loss: 0.00043500869651325047\n",
      "step: 2520, loss: 0.0008140312856994569\n",
      "step: 2521, loss: 0.0006615175516344607\n",
      "step: 2522, loss: 0.00016880307521205395\n",
      "step: 2523, loss: 0.0002782895462587476\n",
      "step: 2524, loss: 0.00020068380399607122\n",
      "step: 2525, loss: 8.524708391632885e-05\n",
      "step: 2526, loss: 0.00012031186633976176\n",
      "step: 2527, loss: 0.018327249214053154\n",
      "step: 2528, loss: 0.0002054264914477244\n",
      "step: 2529, loss: 0.00015101575991138816\n",
      "step: 2530, loss: 0.00028118700720369816\n",
      "step: 2531, loss: 0.0005865350249223411\n",
      "step: 2532, loss: 0.00031594556639902294\n",
      "step: 2533, loss: 0.0006001765141263604\n",
      "step: 2534, loss: 0.00038813866558484733\n",
      "step: 2535, loss: 0.0001318410359090194\n",
      "step: 2536, loss: 0.00019314319069962949\n",
      "step: 2537, loss: 0.00017275761638302356\n",
      "step: 2538, loss: 0.00019490707200020552\n",
      "step: 2539, loss: 0.0001447259564884007\n",
      "step: 2540, loss: 0.02503970079123974\n",
      "step: 2541, loss: 0.00046530357212759554\n",
      "step: 2542, loss: 0.00014414949691854417\n",
      "step: 2543, loss: 0.0003662278177216649\n",
      "step: 2544, loss: 0.00018713081954047084\n",
      "step: 2545, loss: 0.00022498217003885657\n",
      "step: 2546, loss: 0.004267002921551466\n",
      "step: 2547, loss: 0.00019419752061367035\n",
      "step: 2548, loss: 0.00047234262456186116\n",
      "step: 2549, loss: 0.029794270172715187\n",
      "step: 2550, loss: 0.0002043354179477319\n",
      "step: 2551, loss: 0.00032765764626674354\n",
      "step: 2552, loss: 0.00019493669969961047\n",
      "step: 2553, loss: 0.0002641725295688957\n",
      "step: 2554, loss: 0.0003109788813162595\n",
      "step: 2555, loss: 0.031008830294013023\n",
      "step: 2556, loss: 0.0004826111253350973\n",
      "step: 2557, loss: 0.0003875332768075168\n",
      "step: 2558, loss: 0.00026225249166600406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2559, loss: 0.00021861871937289834\n",
      "step: 2560, loss: 0.0004891320713795722\n",
      "step: 2561, loss: 0.0007521420484408736\n",
      "step: 2562, loss: 0.00013467270764522254\n",
      "step: 2563, loss: 0.00021593373094219714\n",
      "step: 2564, loss: 0.00013893033610656857\n",
      "step: 2565, loss: 0.00021371433103922755\n",
      "step: 2566, loss: 0.0002362833038205281\n",
      "step: 2567, loss: 0.00019573904864955693\n",
      "step: 2568, loss: 0.0006214980967342854\n",
      "step: 2569, loss: 0.00015584201901219785\n",
      "step: 2570, loss: 0.00022658945817966014\n",
      "step: 2571, loss: 0.0004140192177146673\n",
      "step: 2572, loss: 0.00017305112851317972\n",
      "step: 2573, loss: 0.00019195425556972623\n",
      "step: 2574, loss: 0.0021482028532773256\n",
      "step: 2575, loss: 0.00020189976203255355\n",
      "step: 2576, loss: 0.02580142579972744\n",
      "step: 2577, loss: 0.0003304421261418611\n",
      "step: 2578, loss: 0.001070054597221315\n",
      "step: 2579, loss: 0.0003430454817134887\n",
      "step: 2580, loss: 0.0003316180082038045\n",
      "step: 2581, loss: 0.0001503563835285604\n",
      "step: 2582, loss: 0.000518892309628427\n",
      "step: 2583, loss: 0.0002953631628770381\n",
      "step: 2584, loss: 0.0001799311430659145\n",
      "step: 2585, loss: 0.00024288937856908888\n",
      "step: 2586, loss: 0.00019971495203208178\n",
      "step: 2587, loss: 0.0001206073138746433\n",
      "step: 2588, loss: 0.0001661182614043355\n",
      "step: 2589, loss: 0.00016495260933879763\n",
      "step: 2590, loss: 0.00019195224740542471\n",
      "step: 2591, loss: 0.00023178804258350283\n",
      "step: 2592, loss: 0.0006395127857103944\n",
      "step: 2593, loss: 0.019347788766026497\n",
      "step: 2594, loss: 0.00021719589130952954\n",
      "step: 2595, loss: 0.00028016793658025563\n",
      "step: 2596, loss: 0.0006505623459815979\n",
      "step: 2597, loss: 0.00033781095407903194\n",
      "step: 2598, loss: 0.0002848582516890019\n",
      "step: 2599, loss: 0.00024135754210874438\n",
      "step: 2600, loss: 0.0001715701655484736\n",
      "step: 2601, loss: 0.0001588072773301974\n",
      "step: 2602, loss: 0.00013931510329712182\n",
      "step: 2603, loss: 0.00020544457947835326\n",
      "step: 2604, loss: 0.0004672406939789653\n",
      "step: 2605, loss: 0.0005254375864751637\n",
      "step: 2606, loss: 0.0006455014809034765\n",
      "step: 2607, loss: 8.485352009302005e-05\n",
      "step: 2608, loss: 0.07543014734983444\n",
      "step: 2609, loss: 0.00016174677875824273\n",
      "step: 2610, loss: 9.159159526461735e-05\n",
      "step: 2611, loss: 0.0001708294148556888\n",
      "step: 2612, loss: 0.0002379204088356346\n",
      "step: 2613, loss: 0.00019559651263989508\n",
      "step: 2614, loss: 0.00026862474624067545\n",
      "step: 2615, loss: 0.0002791301521938294\n",
      "step: 2616, loss: 0.0001794569252524525\n",
      "step: 2617, loss: 0.00015384942525997758\n",
      "step: 2618, loss: 0.0003101679321844131\n",
      "step: 2619, loss: 0.00022046010417398065\n",
      "step: 2620, loss: 0.00019862524641212076\n",
      "step: 2621, loss: 0.010511376895010471\n",
      "step: 2622, loss: 0.00014364019443746656\n",
      "step: 2623, loss: 0.00017664098413661122\n",
      "step: 2624, loss: 9.97451861621812e-05\n",
      "step: 2625, loss: 0.00012297504872549325\n",
      "step: 2626, loss: 0.0001983386027859524\n",
      "step: 2627, loss: 0.00029978094971738756\n",
      "step: 2628, loss: 0.00038640759885311127\n",
      "step: 2629, loss: 0.000276353326626122\n",
      "step: 2630, loss: 0.05387825518846512\n",
      "step: 2631, loss: 0.0001581698888912797\n",
      "step: 2632, loss: 0.0001512451854068786\n",
      "step: 2633, loss: 0.00045126903569325805\n",
      "step: 2634, loss: 0.0002801083610393107\n",
      "step: 2635, loss: 0.0005871859611943364\n",
      "step: 2636, loss: 0.000312922231387347\n",
      "step: 2637, loss: 0.0003590448177419603\n",
      "step: 2638, loss: 0.00027697303448803723\n",
      "step: 2639, loss: 0.0003890997904818505\n",
      "step: 2640, loss: 0.00019063791842199862\n",
      "step: 2641, loss: 0.0003300937241874635\n",
      "step: 2642, loss: 0.0003127447562292218\n",
      "step: 2643, loss: 0.00038017798215150833\n",
      "step: 2644, loss: 0.00026948307640850544\n",
      "step: 2645, loss: 0.00032277259742841125\n",
      "step: 2646, loss: 0.09314660727977753\n",
      "step: 2647, loss: 0.00018106056086253375\n",
      "step: 2648, loss: 0.0002589290088508278\n",
      "step: 2649, loss: 0.00019562536908779293\n",
      "step: 2650, loss: 0.00014639449364040047\n",
      "step: 2651, loss: 0.00011015769996447489\n",
      "step: 2652, loss: 0.00022351952793542296\n",
      "step: 2653, loss: 0.00023251630773302168\n",
      "step: 2654, loss: 0.00035247596679255366\n",
      "step: 2655, loss: 0.02951442077755928\n",
      "step: 2656, loss: 0.0003080673050135374\n",
      "step: 2657, loss: 0.0005731642013415694\n",
      "step: 2658, loss: 0.0003165433881804347\n",
      "step: 2659, loss: 0.0002266453520860523\n",
      "step: 2660, loss: 0.0001860186894191429\n",
      "step: 2661, loss: 0.018299218267202377\n",
      "step: 2662, loss: 0.00019252557831350714\n",
      "step: 2663, loss: 0.00029068379080854356\n",
      "step: 2664, loss: 0.00016551425505895168\n",
      "step: 2665, loss: 0.0002831204910762608\n",
      "step: 2666, loss: 0.00013328313070815057\n",
      "step: 2667, loss: 0.0009420124697498977\n",
      "step: 2668, loss: 0.00023792612773831934\n",
      "step: 2669, loss: 0.0004182962002232671\n",
      "step: 2670, loss: 9.106448123930022e-05\n",
      "step: 2671, loss: 0.00022299606644082814\n",
      "step: 2672, loss: 0.03664395958185196\n",
      "step: 2673, loss: 0.00036062515573576093\n",
      "step: 2674, loss: 0.015251623466610909\n",
      "step: 2675, loss: 0.0001468542031943798\n",
      "step: 2676, loss: 9.372588101541623e-05\n",
      "step: 2677, loss: 0.0004912146250717342\n",
      "step: 2678, loss: 0.0003502668405417353\n",
      "step: 2679, loss: 0.00024696995387785137\n",
      "step: 2680, loss: 0.00011231600365135819\n",
      "step: 2681, loss: 0.0010163808474317193\n",
      "step: 2682, loss: 0.02428322099149227\n",
      "step: 2683, loss: 0.0007148933364078403\n",
      "step: 2684, loss: 0.03019275702536106\n",
      "step: 2685, loss: 0.00046219115029089153\n",
      "step: 2686, loss: 0.0003286852443125099\n",
      "step: 2687, loss: 0.0001885567617136985\n",
      "step: 2688, loss: 0.0003465584013611078\n",
      "step: 2689, loss: 0.00011533748329384252\n",
      "step: 2690, loss: 0.02830333448946476\n",
      "step: 2691, loss: 0.0002909589384216815\n",
      "step: 2692, loss: 0.0006225636461749673\n",
      "step: 2693, loss: 0.0002888482413254678\n",
      "step: 2694, loss: 0.001052855746820569\n",
      "step: 2695, loss: 0.00013211376790422946\n",
      "step: 2696, loss: 0.0001674712257226929\n",
      "step: 2697, loss: 0.04301701858639717\n",
      "step: 2698, loss: 0.0004397500306367874\n",
      "step: 2699, loss: 0.00048216915456578135\n",
      "step: 2700, loss: 0.00016053182480391115\n",
      "step: 2701, loss: 0.0005056493682786822\n",
      "step: 2702, loss: 0.0022931047715246677\n",
      "step: 2703, loss: 0.0001180046601803042\n",
      "step: 2704, loss: 0.0001790938840713352\n",
      "step: 2705, loss: 0.00016810560191515833\n",
      "step: 2706, loss: 0.016629502177238464\n",
      "step: 2707, loss: 0.0007363262702710927\n",
      "step: 2708, loss: 0.00023898354265838861\n",
      "step: 2709, loss: 0.00021479131828527898\n",
      "step: 2710, loss: 0.014905751682817936\n",
      "step: 2711, loss: 0.0003060687449760735\n",
      "step: 2712, loss: 0.0001588994637131691\n",
      "step: 2713, loss: 0.0003046693454962224\n",
      "step: 2714, loss: 7.46725345379673e-05\n",
      "step: 2715, loss: 0.0001620480470592156\n",
      "step: 2716, loss: 0.00047957702190615237\n",
      "step: 2717, loss: 0.00012324296403676271\n",
      "step: 2718, loss: 0.00024107696663122624\n",
      "step: 2719, loss: 0.00025103866937570274\n",
      "step: 2720, loss: 0.00015872377844061702\n",
      "step: 2721, loss: 0.0002685313520487398\n",
      "step: 2722, loss: 0.00030667969258502126\n",
      "step: 2723, loss: 0.00014512399502564222\n",
      "step: 2724, loss: 0.00011447618453530595\n",
      "step: 2725, loss: 0.0001540933590149507\n",
      "step: 2726, loss: 0.00031989251147024333\n",
      "step: 2727, loss: 0.00016405600763391703\n",
      "step: 2728, loss: 0.0001213632058352232\n",
      "step: 2729, loss: 0.0007226192974485457\n",
      "step: 2730, loss: 0.00026170711498707533\n",
      "step: 2731, loss: 0.00014016723434906453\n",
      "step: 2732, loss: 0.000380846846383065\n",
      "step: 2733, loss: 0.00023415505711454898\n",
      "step: 2734, loss: 0.00012911316298414022\n",
      "step: 2735, loss: 0.00015913444804027677\n",
      "step: 2736, loss: 0.00014156602264847606\n",
      "step: 2737, loss: 0.0006286007119342685\n",
      "step: 2738, loss: 0.0002811557496897876\n",
      "step: 2739, loss: 0.00012131101539125666\n",
      "step: 2740, loss: 0.0002964079612866044\n",
      "step: 2741, loss: 0.0002024853602051735\n",
      "step: 2742, loss: 0.00023980582773219794\n",
      "step: 2743, loss: 0.00014729397662449628\n",
      "step: 2744, loss: 0.0003479375736787915\n",
      "step: 2745, loss: 0.00043222890235483646\n",
      "step: 2746, loss: 0.000171426945598796\n",
      "step: 2747, loss: 0.023493126034736633\n",
      "step: 2748, loss: 0.00025450217071920633\n",
      "step: 2749, loss: 0.0001906692486954853\n",
      "step: 2750, loss: 0.0001718073763186112\n",
      "step: 2751, loss: 7.339173316722736e-05\n",
      "step: 2752, loss: 0.0001027642356348224\n",
      "step: 2753, loss: 0.00015193424769677222\n",
      "step: 2754, loss: 8.87464702827856e-05\n",
      "step: 2755, loss: 0.00015533476835116744\n",
      "step: 2756, loss: 0.00025213093613274395\n",
      "step: 2757, loss: 0.0002216220018453896\n",
      "step: 2758, loss: 0.00019134975445922464\n",
      "step: 2759, loss: 0.0011168783530592918\n",
      "step: 2760, loss: 9.220227366313338e-05\n",
      "step: 2761, loss: 0.0004959531361237168\n",
      "step: 2762, loss: 7.142532558646053e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2763, loss: 0.00018971279496327043\n",
      "step: 2764, loss: 7.605714199598879e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-40ed1aa5d041>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m                 \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                 \u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_data_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m             )\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-40ed1aa5d041>\u001b[0m in \u001b[0;36msave_parameters\u001b[1;34m(name, value, epoch)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mstring_form\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m','\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnumber\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mstring_form\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnumber\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./saved-paras/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-40ed1aa5d041>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mstring_form\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m','\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnumber\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mstring_form\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnumber\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./saved-paras/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-40ed1aa5d041>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mstring_form\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m','\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnumber\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mstring_form\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnumber\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./saved-paras/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def load_dataset():\n",
    "    train_data_reader = DataReader(\n",
    "        data_path = './datasets/20news-train-tfidf.txt',\n",
    "        batch_size = 50,\n",
    "        vocab_size = vocab_size\n",
    "    )\n",
    "    test_data_reader = DataReader(\n",
    "        data_path = './datasets/20news-test-tfidf.txt',\n",
    "        batch_size = 50,\n",
    "        vocab_size = vocab_size\n",
    "    )\n",
    "    return train_data_reader, test_data_reader\n",
    "\n",
    "def save_parameters(name, value, epoch):\n",
    "    filename = name.replace(':', '-colon-') + '-epoch-{}.txt'.format(epoch)\n",
    "    if len(value.shape) == 1:\n",
    "        string_form = ','.join([str(number) for number in value])\n",
    "    else:\n",
    "        string_form = '\\n'.join([','.join([str(number) for number in value[row]]) for row in range(value.shape[0])])\n",
    "    \n",
    "    with open('./saved-paras/' + filename, 'w') as f:\n",
    "        f.write(string_form)\n",
    "        \n",
    "def restore_parameters(name, epoch):\n",
    "    filename = name.replace(':', '-colon-') + '-epoch-{}.txt'.format(epoch)\n",
    "    with open('./saved-paras/' + filename) as f:\n",
    "        lines = f.read().splitlines()\n",
    "    if len(lines) == 1:\n",
    "        value = [float(number) for number in lines[0].split(',')]\n",
    "    else:\n",
    "        value = [[float(number) for number in lines[row].split(',')] for row in range(len(lines))]\n",
    "\n",
    "    return value\n",
    "\n",
    "\n",
    "\n",
    "# create computation graph\n",
    "with open('./datasets/words_idfs.txt') as f:\n",
    "    vocab_size = len(f.read().splitlines())\n",
    "    \n",
    "mlp = MLP(\n",
    "    vocab_size = vocab_size,\n",
    "    hidden_size = 50\n",
    ")\n",
    "predicted_labels, loss = mlp.build_graph()\n",
    "train_op = mlp.trainer(loss = loss, learning_rate=0.1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train_data_reader, test_data_reader = load_dataset()\n",
    "    step, MAX_STEP = 0, 1000 ** 2\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "#     # restore params\n",
    "#     trainable_variables = tf.trainable_variables()\n",
    "#     for variable in trainable_variables:\n",
    "#         saved_value = restore_parameters(variable.name, epoch)\n",
    "#         assign_op = variable.assign(saved_value)\n",
    "#         sess.run(assign_op)\n",
    "    \n",
    "    while step < MAX_STEP:\n",
    "        train_data, train_labels = train_data_reader.next_batch()\n",
    "        plabels_eval, loss_eval, _ = sess.run(\n",
    "            [predicted_labels, loss, train_op],\n",
    "            feed_dict = {\n",
    "                mlp._X: train_data,\n",
    "                mlp._real_Y: train_labels\n",
    "            }\n",
    "        )\n",
    "        step += 1\n",
    "        print(\"step: {}, loss: {}\".format(step, loss_eval))\n",
    "        \n",
    "        # save params\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        for variable in trainable_variables:\n",
    "            save_parameters(\n",
    "                name=variable.name,\n",
    "                value=variable.eval(),\n",
    "                epoch=train_data_reader._num_epoch\n",
    "            )\n",
    "            \n",
    "\n",
    "# evaluate model\n",
    "test_data_reader = DataReader(\n",
    "    data_path = './datasets/20news-test_tfidf.txt',\n",
    "    batch_size = 50,\n",
    "    vocab_size = vocab_size\n",
    ")\n",
    "with tf.Session() as sess:\n",
    "    epoch = 10\n",
    "    \n",
    "    trainable_variables = tf.trainable_variables()\n",
    "    for variable in trainable_variables:\n",
    "        saved_value = restore_parameters(variables.name, epoch)\n",
    "        assign_op = variable.assign(saved_value)\n",
    "        sess.run(assign_op)\n",
    "        \n",
    "    num_true_preds = 0\n",
    "    while True:\n",
    "        test_data, test_labels = test_data_reader.next_batch()\n",
    "        test_plabels_eval = sess.run(\n",
    "            predicted_labels,\n",
    "            feed_dict={\n",
    "                mlp._X: test_data,\n",
    "                mlp._real_Y: test_labels\n",
    "            }\n",
    "        )\n",
    "        matches = np.equal(test_plabels_eval, test_labels)\n",
    "        num_true_preds += np.sum(matches.astype(float))\n",
    "        \n",
    "        if test_data_reader._batch_id == 0:\n",
    "            break\n",
    "            \n",
    "    print('Epoch: ', epoch)\n",
    "    print('Accuracy on test data: ', num_true_preds / len(test_data_reader._data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9\n",
      "Accuracy on test data:  0.7469463621879979\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# evaluate model\n",
    "# ??? different vocab_size?\n",
    "test_data_reader = DataReader(\n",
    "    data_path = './datasets/20news-test-tfidf.txt',\n",
    "    batch_size = 50,\n",
    "    vocab_size = vocab_size\n",
    ")\n",
    "with tf.Session() as sess:\n",
    "    epoch = 9\n",
    "    \n",
    "    trainable_variables = tf.trainable_variables()\n",
    "    for variable in trainable_variables:\n",
    "        saved_value = restore_parameters(variable.name, epoch)\n",
    "        assign_op = variable.assign(saved_value)\n",
    "        sess.run(assign_op)\n",
    "        \n",
    "    num_true_preds = 0\n",
    "    while True:\n",
    "        test_data, test_labels = test_data_reader.next_batch()\n",
    "        test_plabels_eval = sess.run(\n",
    "            predicted_labels,\n",
    "            feed_dict={\n",
    "                mlp._X: test_data,\n",
    "                mlp._real_Y: test_labels\n",
    "            }\n",
    "        )\n",
    "        matches = np.equal(test_plabels_eval, test_labels)\n",
    "        num_true_preds += np.sum(matches.astype(float))\n",
    "        \n",
    "        if test_data_reader._batch_id == 0:\n",
    "            break\n",
    "            \n",
    "    print('Epoch: ', epoch)\n",
    "    print('Accuracy on test data: ', num_true_preds / len(test_data_reader._data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
